{
 "metadata": {
  "name": "GaussianProcessClassification"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# \u4f7f\u7528[gaussian process \u5b9e\u73b0\u5206\u7c7b](http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.html)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\nfrom scipy import stats\nfrom sklearn.gaussian_process import GaussianProcess\nfrom matplotlib import  pyplot as plt\nfrom matplotlib import  cm\nfrom sympy import *\nfrom sympy.abc import *\nimport inspect",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Attempted relative import in non-package",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-a76d80cfbb17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#import gdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanhattan_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Attempted relative import in non-package"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "\u6807\u51c6\u5206\u5e03\u51fd\u6570"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "phi = stats.distributions.norm().pdf\nPHI = stats.distributions.norm().cdf\nPHIinv = stats.distributions.norm().ppf",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "\u8fd9\u91cc\u6307\u7684\u662f\u4ec0\u4e48\uff1fpdf, cdf ,ppf?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "lim =8\ninspect.getsourcefile(GaussianProcess)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": "'/usr/local/lib/python2.7/dist-packages/sklearn/gaussian_process/gaussian_process.py'"
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def g(x):\n    return 5.-x[:,1]-.5*x[:,0]**2",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "def \u7684\u51fd\u6570\u600e\u6837\u663e\u793a\u51fa\u6765\uff1f"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "X = np.array([[-4.61611719, -6.00099547],\n              [4.10469096, 5.32782448],\n              [0.00000000, -0.50000000],\n              [-6.17289014, -4.6984743],\n              [1.3109306, -6.93271427],\n              [-5.03823144, 3.10584743],\n              [-2.87600388, 6.74310541],\n              [5.21301203, 4.26386883]])\nX= np.random.randn(8,8)\nprint X.shape\nprint X.shape[1]",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "(8, 8)\n8\n"
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "\u8fd9\u91cc\u600e\u6837\u62df\u5408\u4e8c\u7ef4\u6570\u636e\uff1f \u8fd9\u80fd\u53ea\u80fd\u5904\u7406\u4e24\u4e2a\u7279\u6027\uff0c\u66f4\u591a\u7684\u7279\u6027\u62df\u5408\u4e0d\u51fa\u6765\u3002"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from __future__ import print_function\n\nimport numpy as np\nfrom scipy import linalg, optimize\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics.pairwise import manhattan_distances\nfrom sklearn.utils  import check_random_state, check_array, check_X_y\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn import  linear_model",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "\n\nMACHINE_EPSILON = np.finfo(np.double).eps\ndef l1_cross_distances(X):\n    \"\"\"\n    Computes the nonzero componentwise L1 cross-distances between the vectors\n    in X.\n    Parameters\n    ----------\n    X: array_like\n        An array with shape (n_samples, n_features)\n    Returns\n    -------\n    D: array with shape (n_samples * (n_samples - 1) / 2, n_features)\n        The array of componentwise L1 cross-distances.\n    ij: arrays with shape (n_samples * (n_samples - 1) / 2, 2)\n        The indices i and j of the vectors in X associated to the cross-\n        distances in D: D[k] = np.abs(X[ij[k, 0]] - Y[ij[k, 1]]).\n    \"\"\"\n    X = check_array(X)\n    n_samples, n_features = X.shape\n    n_nonzero_cross_dist = n_samples * (n_samples - 1) // 2\n    ij = np.zeros((n_nonzero_cross_dist, 2), dtype=np.int)\n    D = np.zeros((n_nonzero_cross_dist, n_features))\n    ll_1 = 0\n    for k in range(n_samples - 1):\n        ll_0 = ll_1\n        ll_1 = ll_0 + n_samples - k - 1\n        ij[ll_0:ll_1, 0] = k\n        ij[ll_0:ll_1, 1] = np.arange(k + 1, n_samples)\n        D[ll_0:ll_1] = np.abs(X[k] - X[(k + 1):n_samples])\n\n    return D, ij\n\n\nclass GaussianProcess(BaseEstimator, RegressorMixin):\n    \"\"\"The Gaussian Process model class.\n    Parameters\n    ----------\n    regr : string or callable, optional\n        A regression function returning an array of outputs of the linear\n        regression functional basis. The number of observations n_samples\n        should be greater than the size p of this basis.\n        Default assumes a simple constant regression trend.\n        Available built-in regression models are::\n            'constant', 'linear', 'quadratic'\n    corr : string or callable, optional\n        A stationary autocorrelation function returning the autocorrelation\n        between two points x and x'.\n        Default assumes a squared-exponential autocorrelation model.\n        Built-in correlation models are::\n            'absolute_exponential', 'squared_exponential',\n            'generalized_exponential', 'cubic', 'linear'\n    beta0 : double array_like, optional\n        The regression weight vector to perform Ordinary Kriging (OK).\n        Default assumes Universal Kriging (UK) so that the vector beta of\n        regression weights is estimated using the maximum likelihood\n        principle.\n    storage_mode : string, optional\n        A string specifying whether the Cholesky decomposition of the\n        correlation matrix should be stored in the class (storage_mode =\n        'full') or not (storage_mode = 'light').\n        Default assumes storage_mode = 'full', so that the\n        Cholesky decomposition of the correlation matrix is stored.\n        This might be a useful parameter when one is not interested in the\n        MSE and only plan to estimate the BLUP, for which the correlation\n        matrix is not required.\n    verbose : boolean, optional\n        A boolean specifying the verbose level.\n        Default is verbose = False.\n    theta0 : double array_like, optional\n        An array with shape (n_features, ) or (1, ).\n        The parameters in the autocorrelation model.\n        If thetaL and thetaU are also specified, theta0 is considered as\n        the starting point for the maximum likelihood estimation of the\n        best set of parameters.\n        Default assumes isotropic autocorrelation model with theta0 = 1e-1.\n    thetaL : double array_like, optional\n        An array with shape matching theta0's.\n        Lower bound on the autocorrelation parameters for maximum\n        likelihood estimation.\n        Default is None, so that it skips maximum likelihood estimation and\n        it uses theta0.\n    thetaU : double array_like, optional\n        An array with shape matching theta0's.\n        Upper bound on the autocorrelation parameters for maximum\n        likelihood estimation.\n        Default is None, so that it skips maximum likelihood estimation and\n        it uses theta0.\n    normalize : boolean, optional\n        Input X and observations y are centered and reduced wrt\n        means and standard deviations estimated from the n_samples\n        observations provided.\n        Default is normalize = True so that data is normalized to ease\n        maximum likelihood estimation.\n    nugget : double or ndarray, optional\n        Introduce a nugget effect to allow smooth predictions from noisy\n        data.  If nugget is an ndarray, it must be the same length as the\n        number of data points used for the fit.\n        The nugget is added to the diagonal of the assumed training covariance;\n        in this way it acts as a Tikhonov regularization in the problem.  In\n        the special case of the squared exponential correlation function, the\n        nugget mathematically represents the variance of the input values.\n        Default assumes a nugget close to machine precision for the sake of\n        robustness (nugget = 10. * MACHINE_EPSILON).\n    optimizer : string, optional\n        A string specifying the optimization algorithm to be used.\n        Default uses 'fmin_cobyla' algorithm from scipy.optimize.\n        Available optimizers are::\n            'fmin_cobyla', 'Welch'\n        'Welch' optimizer is dued to Welch et al., see reference [WBSWM1992]_.\n        It consists in iterating over several one-dimensional optimizations\n        instead of running one single multi-dimensional optimization.\n    random_start : int, optional\n        The number of times the Maximum Likelihood Estimation should be\n        performed from a random starting point.\n        The first MLE always uses the specified starting point (theta0),\n        the next starting points are picked at random according to an\n        exponential distribution (log-uniform on [thetaL, thetaU]).\n        Default does not use random starting point (random_start = 1).\n    random_state: integer or numpy.RandomState, optional\n        The generator used to shuffle the sequence of coordinates of theta in\n        the Welch optimizer. If an integer is given, it fixes the seed.\n        Defaults to the global numpy random number generator.\n    Attributes\n    ----------\n    theta_ : array\n        Specified theta OR the best set of autocorrelation parameters (the \\\n        sought maximizer of the reduced likelihood function).\n    reduced_likelihood_function_value_ : array\n        The optimal reduced likelihood function value.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.gaussian_process import GaussianProcess\n    >>> X = np.array([[1., 3., 5., 6., 7., 8.]]).T\n    >>> y = (X * np.sin(X)).ravel()\n    >>> gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.)\n    >>> gp.fit(X, y)                                      # doctest: +ELLIPSIS\n    GaussianProcess(beta0=None...\n            ...\n    Notes\n    -----\n    The presentation implementation is based on a translation of the DACE\n    Matlab toolbox, see reference [NLNS2002]_.\n    References\n    ----------\n    .. [NLNS2002] `H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.\n        Sondergaard.  DACE - A MATLAB Kriging Toolbox.` (2002)\n        http://www2.imm.dtu.dk/~hbn/dace/dace.pdf\n    .. [WBSWM1992] `W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,\n        and M.D.  Morris (1992). Screening, predicting, and computer\n        experiments.  Technometrics, 34(1) 15--25.`\n        http://www.jstor.org/pss/1269548\n    \"\"\"\n\n    _regression_types = {\n        'constant': regression.constant,\n        'linear': regression.linear,\n        'quadratic': regression.quadratic}\n\n    _correlation_types = {\n        'absolute_exponential': correlation.absolute_exponential,\n        'squared_exponential': correlation.squared_exponential,\n        'generalized_exponential': correlation.generalized_exponential,\n        'cubic': correlation.cubic,\n        'linear': correlation.linear}\n\n    _optimizer_types = [\n        'fmin_cobyla',\n        'Welch']\n\n    def __init__(self, regr='constant', corr='squared_exponential', beta0=None,\n                 storage_mode='full', verbose=False, theta0=1e-1,\n                 thetaL=None, thetaU=None, optimizer='fmin_cobyla',\n                 random_start=1, normalize=True,\n                 nugget=10. * MACHINE_EPSILON, random_state=None):\n\n        self.regr = regr\n        self.corr = corr\n        self.beta0 = beta0\n        self.storage_mode = storage_mode\n        self.verbose = verbose\n        self.theta0 = theta0\n        self.thetaL = thetaL\n        self.thetaU = thetaU\n        self.normalize = normalize\n        self.nugget = nugget\n        self.optimizer = optimizer\n        self.random_start = random_start\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"\n        The Gaussian Process model fitting method.\n        Parameters\n        ----------\n        X : double array_like\n            An array with shape (n_samples, n_features) with the input at which\n            observations were made.\n        y : double array_like\n            An array with shape (n_samples, ) or shape (n_samples, n_targets)\n            with the observations of the output to be predicted.\n        Returns\n        -------\n        gp : self\n            A fitted Gaussian Process model object awaiting data to perform\n            predictions.\n        \"\"\"\n        # Run input checks\n        self._check_params()\n\n        self.random_state = check_random_state(self.random_state)\n\n        # Force data to 2D numpy.array\n        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)\n        self.y_ndim_ = y.ndim\n        if y.ndim == 1:\n            y = y[:, np.newaxis]\n\n        # Check shapes of DOE & observations\n        n_samples, n_features = X.shape\n        _, n_targets = y.shape\n\n        # Run input checks\n        self._check_params(n_samples)\n\n        # Normalize data or don't\n        if self.normalize:\n            X_mean = np.mean(X, axis=0)\n            X_std = np.std(X, axis=0)\n            y_mean = np.mean(y, axis=0)\n            y_std = np.std(y, axis=0)\n            X_std[X_std == 0.] = 1.\n            y_std[y_std == 0.] = 1.\n            # center and scale X if necessary\n            X = (X - X_mean) / X_std\n            y = (y - y_mean) / y_std\n        else:\n            X_mean = np.zeros(1)\n            X_std = np.ones(1)\n            y_mean = np.zeros(1)\n            y_std = np.ones(1)\n\n        # Calculate matrix of distances D between samples\n        D, ij = l1_cross_distances(X)\n        if (np.min(np.sum(D, axis=1)) == 0.\n                and self.corr != correlation.pure_nugget):\n            raise Exception(\"Multiple input features cannot have the same\"\n                            \" target value.\")\n\n        # Regression matrix and parameters\n        F = self.regr(X)\n        n_samples_F = F.shape[0]\n        if F.ndim > 1:\n            p = F.shape[1]\n        else:\n            p = 1\n        if n_samples_F != n_samples:\n            raise Exception(\"Number of rows in F and X do not match. Most \"\n                            \"likely something is going wrong with the \"\n                            \"regression model.\")\n        if p > n_samples_F:\n            raise Exception((\"Ordinary least squares problem is undetermined \"\n                             \"n_samples=%d must be greater than the \"\n                             \"regression model size p=%d.\") % (n_samples, p))\n        if self.beta0 is not None:\n            if self.beta0.shape[0] != p:\n                raise Exception(\"Shapes of beta0 and F do not match.\")\n\n        # Set attributes\n        self.X = X\n        self.y = y\n        self.D = D\n        self.ij = ij\n        self.F = F\n        self.X_mean, self.X_std = X_mean, X_std\n        self.y_mean, self.y_std = y_mean, y_std\n\n        # Determine Gaussian Process model parameters\n        if self.thetaL is not None and self.thetaU is not None:\n            # Maximum Likelihood Estimation of the parameters\n            if self.verbose:\n                print(\"Performing Maximum Likelihood Estimation of the \"\n                      \"autocorrelation parameters...\")\n            self.theta_, self.reduced_likelihood_function_value_, par = \\\n                self._arg_max_reduced_likelihood_function()\n            if np.isinf(self.reduced_likelihood_function_value_):\n                raise Exception(\"Bad parameter region. \"\n                                \"Try increasing upper bound\")\n\n        else:\n            # Given parameters\n            if self.verbose:\n                print(\"Given autocorrelation parameters. \"\n                      \"Computing Gaussian Process model parameters...\")\n            self.theta_ = self.theta0\n            self.reduced_likelihood_function_value_, par = \\\n                self.reduced_likelihood_function()\n            if np.isinf(self.reduced_likelihood_function_value_):\n                raise Exception(\"Bad point. Try increasing theta0.\")\n\n        self.beta = par['beta']\n        self.gamma = par['gamma']\n        self.sigma2 = par['sigma2']\n        self.C = par['C']\n        self.Ft = par['Ft']\n        self.G = par['G']\n\n        if self.storage_mode == 'light':\n            # Delete heavy data (it will be computed again if required)\n            # (it is required only when MSE is wanted in self.predict)\n            if self.verbose:\n                print(\"Light storage mode specified. \"\n                      \"Flushing autocorrelation matrix...\")\n            self.D = None\n            self.ij = None\n            self.F = None\n            self.C = None\n            self.Ft = None\n            self.G = None\n\n        return self\n\n    def predict(self, X, eval_MSE=False, batch_size=None):\n        \"\"\"\n        This function evaluates the Gaussian Process model at x.\n        Parameters\n        ----------\n        X : array_like\n            An array with shape (n_eval, n_features) giving the point(s) at\n            which the prediction(s) should be made.\n        eval_MSE : boolean, optional\n            A boolean specifying whether the Mean Squared Error should be\n            evaluated or not.\n            Default assumes evalMSE = False and evaluates only the BLUP (mean\n            prediction).\n        batch_size : integer, optional\n            An integer giving the maximum number of points that can be\n            evaluated simultaneously (depending on the available memory).\n            Default is None so that all given points are evaluated at the same\n            time.\n        Returns\n        -------\n        y : array_like, shape (n_samples, ) or (n_samples, n_targets)\n            An array with shape (n_eval, ) if the Gaussian Process was trained\n            on an array of shape (n_samples, ) or an array with shape\n            (n_eval, n_targets) if the Gaussian Process was trained on an array\n            of shape (n_samples, n_targets) with the Best Linear Unbiased\n            Prediction at x.\n        MSE : array_like, optional (if eval_MSE == True)\n            An array with shape (n_eval, ) or (n_eval, n_targets) as with y,\n            with the Mean Squared Error at x.\n        \"\"\"\n        check_is_fitted(self, \"X\")\n\n        # Check input shapes\n        X = check_array(X)\n        n_eval, _ = X.shape\n        n_samples, n_features = self.X.shape\n        n_samples_y, n_targets = self.y.shape\n\n        # Run input checks\n        self._check_params(n_samples)\n\n        if X.shape[1] != n_features:\n            raise ValueError((\"The number of features in X (X.shape[1] = %d) \"\n                              \"should match the number of features used \"\n                              \"for fit() \"\n                              \"which is %d.\") % (X.shape[1], n_features))\n\n        if batch_size is None:\n            # No memory management\n            # (evaluates all given points in a single batch run)\n\n            # Normalize input\n            X = (X - self.X_mean) / self.X_std\n\n            # Initialize output\n            y = np.zeros(n_eval)\n            if eval_MSE:\n                MSE = np.zeros(n_eval)\n\n            # Get pairwise componentwise L1-distances to the input training set\n            dx = manhattan_distances(X, Y=self.X, sum_over_features=False)\n            # Get regression function and correlation\n            f = self.regr(X)\n            r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)\n\n            # Scaled predictor\n            y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)\n\n            # Predictor\n            y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)\n\n            if self.y_ndim_ == 1:\n                y = y.ravel()\n\n            # Mean Squared Error\n            if eval_MSE:\n                C = self.C\n                if C is None:\n                    # Light storage mode (need to recompute C, F, Ft and G)\n                    if self.verbose:\n                        print(\"This GaussianProcess used 'light' storage mode \"\n                              \"at instantiation. Need to recompute \"\n                              \"autocorrelation matrix...\")\n                    reduced_likelihood_function_value, par = \\\n                        self.reduced_likelihood_function()\n                    self.C = par['C']\n                    self.Ft = par['Ft']\n                    self.G = par['G']\n\n                rt = linalg.solve_triangular(self.C, r.T, lower=True)\n\n                if self.beta0 is None:\n                    # Universal Kriging\n                    u = linalg.solve_triangular(self.G.T,\n                                                np.dot(self.Ft.T, rt) - f.T,\n                                                lower=True)\n                else:\n                    # Ordinary Kriging\n                    u = np.zeros((n_targets, n_eval))\n\n                MSE = np.dot(self.sigma2.reshape(n_targets, 1),\n                             (1. - (rt ** 2.).sum(axis=0)\n                              + (u ** 2.).sum(axis=0))[np.newaxis, :])\n                MSE = np.sqrt((MSE ** 2.).sum(axis=0) / n_targets)\n\n                # Mean Squared Error might be slightly negative depending on\n                # machine precision: force to zero!\n                MSE[MSE < 0.] = 0.\n\n                if self.y_ndim_ == 1:\n                    MSE = MSE.ravel()\n\n                return y, MSE\n\n            else:\n\n                return y\n\n        else:\n            # Memory management\n\n            if type(batch_size) is not int or batch_size <= 0:\n                raise Exception(\"batch_size must be a positive integer\")\n\n            if eval_MSE:\n\n                y, MSE = np.zeros(n_eval), np.zeros(n_eval)\n                for k in range(max(1, n_eval / batch_size)):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])\n                    y[batch_from:batch_to], MSE[batch_from:batch_to] = \\\n                        self.predict(X[batch_from:batch_to],\n                                     eval_MSE=eval_MSE, batch_size=None)\n\n                return y, MSE\n\n            else:\n\n                y = np.zeros(n_eval)\n                for k in range(max(1, n_eval / batch_size)):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])\n                    y[batch_from:batch_to] = \\\n                        self.predict(X[batch_from:batch_to],\n                                     eval_MSE=eval_MSE, batch_size=None)\n\n                return y\n\n    def reduced_likelihood_function(self, theta=None):\n        \"\"\"\n        This function determines the BLUP parameters and evaluates the reduced\n        likelihood function for the given autocorrelation parameters theta.\n        Maximizing this function wrt the autocorrelation parameters theta is\n        equivalent to maximizing the likelihood of the assumed joint Gaussian\n        distribution of the observations y evaluated onto the design of\n        experiments X.\n        Parameters\n        ----------\n        theta : array_like, optional\n            An array containing the autocorrelation parameters at which the\n            Gaussian Process model parameters should be determined.\n            Default uses the built-in autocorrelation parameters\n            (ie ``theta = self.theta_``).\n        Returns\n        -------\n        reduced_likelihood_function_value : double\n            The value of the reduced likelihood function associated to the\n            given autocorrelation parameters theta.\n        par : dict\n            A dictionary containing the requested Gaussian Process model\n            parameters:\n                sigma2\n                        Gaussian Process variance.\n                beta\n                        Generalized least-squares regression weights for\n                        Universal Kriging or given beta0 for Ordinary\n                        Kriging.\n                gamma\n                        Gaussian Process weights.\n                C\n                        Cholesky decomposition of the correlation matrix [R].\n                Ft\n                        Solution of the linear equation system : [R] x Ft = F\n                G\n                        QR decomposition of the matrix Ft.\n        \"\"\"\n        check_is_fitted(self, \"X\")\n\n        if theta is None:\n            # Use built-in autocorrelation parameters\n            theta = self.theta_\n\n        # Initialize output\n        reduced_likelihood_function_value = - np.inf\n        par = {}\n\n        # Retrieve data\n        n_samples = self.X.shape[0]\n        D = self.D\n        ij = self.ij\n        F = self.F\n\n        if D is None:\n            # Light storage mode (need to recompute D, ij and F)\n            D, ij = l1_cross_distances(self.X)\n            if (np.min(np.sum(D, axis=1)) == 0.\n                    and self.corr != correlation.pure_nugget):\n                raise Exception(\"Multiple X are not allowed\")\n            F = self.regr(self.X)\n\n        # Set up R\n        r = self.corr(theta, D)\n        R = np.eye(n_samples) * (1. + self.nugget)\n        R[ij[:, 0], ij[:, 1]] = r\n        R[ij[:, 1], ij[:, 0]] = r\n\n        # Cholesky decomposition of R\n        try:\n            C = linalg.cholesky(R, lower=True)\n        except linalg.LinAlgError:\n            return reduced_likelihood_function_value, par\n\n        # Get generalized least squares solution\n        Ft = linalg.solve_triangular(C, F, lower=True)\n        try:\n            Q, G = linalg.qr(Ft, econ=True)\n        except:\n            #/usr/lib/python2.6/dist-packages/scipy/linalg/decomp.py:1177:\n            # DeprecationWarning: qr econ argument will be removed after scipy\n            # 0.7. The economy transform will then be available through the\n            # mode='economic' argument.\n            Q, G = linalg.qr(Ft, mode='economic')\n            pass\n\n        sv = linalg.svd(G, compute_uv=False)\n        rcondG = sv[-1] / sv[0]\n        if rcondG < 1e-10:\n            # Check F\n            sv = linalg.svd(F, compute_uv=False)\n            condF = sv[0] / sv[-1]\n            if condF > 1e15:\n                raise Exception(\"F is too ill conditioned. Poor combination \"\n                                \"of regression model and observations.\")\n            else:\n                # Ft is too ill conditioned, get out (try different theta)\n                return reduced_likelihood_function_value, par\n\n        Yt = linalg.solve_triangular(C, self.y, lower=True)\n        if self.beta0 is None:\n            # Universal Kriging\n            beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))\n        else:\n            # Ordinary Kriging\n            beta = np.array(self.beta0)\n\n        rho = Yt - np.dot(Ft, beta)\n        sigma2 = (rho ** 2.).sum(axis=0) / n_samples\n        # The determinant of R is equal to the squared product of the diagonal\n        # elements of its Cholesky decomposition C\n        detR = (np.diag(C) ** (2. / n_samples)).prod()\n\n        # Compute/Organize output\n        reduced_likelihood_function_value = - sigma2.sum() * detR\n        par['sigma2'] = sigma2 * self.y_std ** 2.\n        par['beta'] = beta\n        par['gamma'] = linalg.solve_triangular(C.T, rho)\n        par['C'] = C\n        par['Ft'] = Ft\n        par['G'] = G\n\n        return reduced_likelihood_function_value, par\n\n    def _arg_max_reduced_likelihood_function(self):\n        \"\"\"\n        This function estimates the autocorrelation parameters theta as the\n        maximizer of the reduced likelihood function.\n        (Minimization of the opposite reduced likelihood function is used for\n        convenience)\n        Parameters\n        ----------\n        self : All parameters are stored in the Gaussian Process model object.\n        Returns\n        -------\n        optimal_theta : array_like\n            The best set of autocorrelation parameters (the sought maximizer of\n            the reduced likelihood function).\n        optimal_reduced_likelihood_function_value : double\n            The optimal reduced likelihood function value.\n        optimal_par : dict\n            The BLUP parameters associated to thetaOpt.\n        \"\"\"\n\n        # Initialize output\n        best_optimal_theta = []\n        best_optimal_rlf_value = []\n        best_optimal_par = []\n\n        if self.verbose:\n            print(\"The chosen optimizer is: \" + str(self.optimizer))\n            if self.random_start > 1:\n                print(str(self.random_start) + \" random starts are required.\")\n\n        percent_completed = 0.\n\n        # Force optimizer to fmin_cobyla if the model is meant to be isotropic\n        if self.optimizer == 'Welch' and self.theta0.size == 1:\n            self.optimizer = 'fmin_cobyla'\n\n        if self.optimizer == 'fmin_cobyla':\n\n            def minus_reduced_likelihood_function(log10t):\n                return - self.reduced_likelihood_function(\n                    theta=10. ** log10t)[0]\n\n            constraints = []\n            for i in range(self.theta0.size):\n                constraints.append(lambda log10t, i=i:\n                                   log10t[i] - np.log10(self.thetaL[0, i]))\n                constraints.append(lambda log10t, i=i:\n                                   np.log10(self.thetaU[0, i]) - log10t[i])\n\n            for k in range(self.random_start):\n\n                if k == 0:\n                    # Use specified starting point as first guess\n                    theta0 = self.theta0\n                else:\n                    # Generate a random starting point log10-uniformly\n                    # distributed between bounds\n                    log10theta0 = np.log10(self.thetaL) \\\n                        + self.random_state.rand(self.theta0.size).reshape(\n                            self.theta0.shape) * np.log10(self.thetaU\n                                                          / self.thetaL)\n                    theta0 = 10. ** log10theta0\n\n                # Run Cobyla\n                try:\n                    log10_optimal_theta = \\\n                        optimize.fmin_cobyla(minus_reduced_likelihood_function,\n                                             np.log10(theta0), constraints,\n                                             iprint=0)\n                except ValueError as ve:\n                    print(\"Optimization failed. Try increasing the ``nugget``\")\n                    raise ve\n\n                optimal_theta = 10. ** log10_optimal_theta\n                optimal_rlf_value, optimal_par = \\\n                    self.reduced_likelihood_function(theta=optimal_theta)\n\n                # Compare the new optimizer to the best previous one\n                if k > 0:\n                    if optimal_rlf_value > best_optimal_rlf_value:\n                        best_optimal_rlf_value = optimal_rlf_value\n                        best_optimal_par = optimal_par\n                        best_optimal_theta = optimal_theta\n                else:\n                    best_optimal_rlf_value = optimal_rlf_value\n                    best_optimal_par = optimal_par\n                    best_optimal_theta = optimal_theta\n                if self.verbose and self.random_start > 1:\n                    if (20 * k) / self.random_start > percent_completed:\n                        percent_completed = (20 * k) / self.random_start\n                        print(\"%s completed\" % (5 * percent_completed))\n\n            optimal_rlf_value = best_optimal_rlf_value\n            optimal_par = best_optimal_par\n            optimal_theta = best_optimal_theta\n\n        elif self.optimizer == 'Welch':\n\n            # Backup of the given atrributes\n            theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU\n            corr = self.corr\n            verbose = self.verbose\n\n            # This will iterate over fmin_cobyla optimizer\n            self.optimizer = 'fmin_cobyla'\n            self.verbose = False\n\n            # Initialize under isotropy assumption\n            if verbose:\n                print(\"Initialize under isotropy assumption...\")\n            self.theta0 = check_array(self.theta0.min())\n            self.thetaL = check_array(self.thetaL.min())\n            self.thetaU = check_array(self.thetaU.max())\n            theta_iso, optimal_rlf_value_iso, par_iso = \\\n                self._arg_max_reduced_likelihood_function()\n            optimal_theta = theta_iso + np.zeros(theta0.shape)\n\n            # Iterate over all dimensions of theta allowing for anisotropy\n            if verbose:\n                print(\"Now improving allowing for anisotropy...\")\n            for i in self.random_state.permutation(theta0.size):\n                if verbose:\n                    print(\"Proceeding along dimension %d...\" % (i + 1))\n                self.theta0 = check_array(theta_iso)\n                self.thetaL = check_array(thetaL[0, i])\n                self.thetaU = check_array(thetaU[0, i])\n\n                def corr_cut(t, d):\n                    return corr(check_array(np.hstack([optimal_theta[0][0:i],\n                                                       t[0],\n                                                       optimal_theta[0][(i +\n                                                                         1)::]])),\n                                d)\n\n                self.corr = corr_cut\n                optimal_theta[0, i], optimal_rlf_value, optimal_par = \\\n                    self._arg_max_reduced_likelihood_function()\n\n            # Restore the given atrributes\n            self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU\n            self.corr = corr\n            self.optimizer = 'Welch'\n            self.verbose = verbose\n\n        else:\n\n            raise NotImplementedError(\"This optimizer ('%s') is not \"\n                                      \"implemented yet. Please contribute!\"\n                                      % self.optimizer)\n\n        return optimal_theta, optimal_rlf_value, optimal_par\n\n    def _check_params(self, n_samples=None):\n\n        # Check regression model\n        if not callable(self.regr):\n            if self.regr in self._regression_types:\n                self.regr = self._regression_types[self.regr]\n            else:\n                raise ValueError(\"regr should be one of %s or callable, \"\n                                 \"%s was given.\"\n                                 % (self._regression_types.keys(), self.regr))\n\n        # Check regression weights if given (Ordinary Kriging)\n        if self.beta0 is not None:\n            self.beta0 = check_array(self.beta0)\n            if self.beta0.shape[1] != 1:\n                # Force to column vector\n                self.beta0 = self.beta0.T\n\n        # Check correlation model\n        if not callable(self.corr):\n            if self.corr in self._correlation_types:\n                self.corr = self._correlation_types[self.corr]\n            else:\n                raise ValueError(\"corr should be one of %s or callable, \"\n                                 \"%s was given.\"\n                                 % (self._correlation_types.keys(), self.corr))\n\n        # Check storage mode\n        if self.storage_mode != 'full' and self.storage_mode != 'light':\n            raise ValueError(\"Storage mode should either be 'full' or \"\n                             \"'light', %s was given.\" % self.storage_mode)\n\n        # Check correlation parameters\n        self.theta0 = check_array(self.theta0)\n        lth = self.theta0.size\n\n        if self.thetaL is not None and self.thetaU is not None:\n            self.thetaL = check_array(self.thetaL)\n            self.thetaU = check_array(self.thetaU)\n            if self.thetaL.size != lth or self.thetaU.size != lth:\n                raise ValueError(\"theta0, thetaL and thetaU must have the \"\n                                 \"same length.\")\n            if np.any(self.thetaL <= 0) or np.any(self.thetaU < self.thetaL):\n                raise ValueError(\"The bounds must satisfy O < thetaL <= \"\n                                 \"thetaU.\")\n\n        elif self.thetaL is None and self.thetaU is None:\n            if np.any(self.theta0 <= 0):\n                raise ValueError(\"theta0 must be strictly positive.\")\n\n        elif self.thetaL is None or self.thetaU is None:\n            raise ValueError(\"thetaL and thetaU should either be both or \"\n                             \"neither specified.\")\n\n        # Force verbose type to bool\n        self.verbose = bool(self.verbose)\n\n        # Force normalize type to bool\n        self.normalize = bool(self.normalize)\n\n        # Check nugget value\n        self.nugget = np.asarray(self.nugget)\n        if np.any(self.nugget) < 0.:\n            raise ValueError(\"nugget must be positive or zero.\")\n        if (n_samples is not None\n                and self.nugget.shape not in [(), (n_samples,)]):\n            raise ValueError(\"nugget must be either a scalar \"\n                             \"or array of length n_samples.\")\n\n        # Check optimizer\n        if self.optimizer not in self._optimizer_types:\n            raise ValueError(\"optimizer should be one of %s\"\n                             % self._optimizer_types)\n\n        # Force random_start type to int\n        self.random_start = int(self.random_start)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'regression' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-e75238b31ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGaussianProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \"\"\"The Gaussian Process model class.\n\u001b[1;32m     38\u001b[0m     \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-6-e75238b31ed9>\u001b[0m in \u001b[0;36mGaussianProcess\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     _regression_types = {\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;34m'constant'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;34m'linear'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         'quadratic': regression.quadratic}\n",
        "\u001b[0;31mNameError\u001b[0m: name 'regression' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "class GaussianProcess(BaseEstimator, RegressorMixin):\n    \"\"\"The Gaussian Process model class.\n\n    Parameters\n    ----------\n    regr : string or callable, optional\n        A regression function returning an array of outputs of the linear\n        regression functional basis. The number of observations n_samples\n        should be greater than the size p of this basis.\n        Default assumes a simple constant regression trend.\n        Available built-in regression models are::\n\n            'constant', 'linear', 'quadratic'\n\n    corr : string or callable, optional\n        A stationary autocorrelation function returning the autocorrelation\n        between two points x and x'.\n        Default assumes a squared-exponential autocorrelation model.\n        Built-in correlation models are::\n\n            'absolute_exponential', 'squared_exponential',\n            'generalized_exponential', 'cubic', 'linear'\n\n    beta0 : double array_like, optional\n        The regression weight vector to perform Ordinary Kriging (OK).\n        Default assumes Universal Kriging (UK) so that the vector beta of\n        regression weights is estimated using the maximum likelihood\n        principle.\n\n    storage_mode : string, optional\n        A string specifying whether the Cholesky decomposition of the\n        correlation matrix should be stored in the class (storage_mode =\n        'full') or not (storage_mode = 'light').\n        Default assumes storage_mode = 'full', so that the\n        Cholesky decomposition of the correlation matrix is stored.\n        This might be a useful parameter when one is not interested in the\n        MSE and only plan to estimate the BLUP, for which the correlation\n        matrix is not required.\n\n    verbose : boolean, optional\n        A boolean specifying the verbose level.\n        Default is verbose = False.\n\n    theta0 : double array_like, optional\n        An array with shape (n_features, ) or (1, ).\n        The parameters in the autocorrelation model.\n        If thetaL and thetaU are also specified, theta0 is considered as\n        the starting point for the maximum likelihood estimation of the\n        best set of parameters.\n        Default assumes isotropic autocorrelation model with theta0 = 1e-1.\n\n    thetaL : double array_like, optional\n        An array with shape matching theta0's.\n        Lower bound on the autocorrelation parameters for maximum\n        likelihood estimation.\n        Default is None, so that it skips maximum likelihood estimation and\n        it uses theta0.\n\n    thetaU : double array_like, optional\n        An array with shape matching theta0's.\n        Upper bound on the autocorrelation parameters for maximum\n        likelihood estimation.\n        Default is None, so that it skips maximum likelihood estimation and\n        it uses theta0.\n\n    normalize : boolean, optional\n        Input X and observations y are centered and reduced wrt\n        means and standard deviations estimated from the n_samples\n        observations provided.\n        Default is normalize = True so that data is normalized to ease\n        maximum likelihood estimation.\n\n    nugget : double or ndarray, optional\n        Introduce a nugget effect to allow smooth predictions from noisy\n        data.  If nugget is an ndarray, it must be the same length as the\n        number of data points used for the fit.\n        The nugget is added to the diagonal of the assumed training covariance;\n        in this way it acts as a Tikhonov regularization in the problem.  In\n        the special case of the squared exponential correlation function, the\n        nugget mathematically represents the variance of the input values.\n        Default assumes a nugget close to machine precision for the sake of\n        robustness (nugget = 10. * MACHINE_EPSILON).\n\n    optimizer : string, optional\n        A string specifying the optimization algorithm to be used.\n        Default uses 'fmin_cobyla' algorithm from scipy.optimize.\n        Available optimizers are::\n\n            'fmin_cobyla', 'Welch'\n\n        'Welch' optimizer is dued to Welch et al., see reference [WBSWM1992]_.\n        It consists in iterating over several one-dimensional optimizations\n        instead of running one single multi-dimensional optimization.\n\n    random_start : int, optional\n        The number of times the Maximum Likelihood Estimation should be\n        performed from a random starting point.\n        The first MLE always uses the specified starting point (theta0),\n        the next starting points are picked at random according to an\n        exponential distribution (log-uniform on [thetaL, thetaU]).\n        Default does not use random starting point (random_start = 1).\n\n    random_state: integer or numpy.RandomState, optional\n        The generator used to shuffle the sequence of coordinates of theta in\n        the Welch optimizer. If an integer is given, it fixes the seed.\n        Defaults to the global numpy random number generator.\n\n\n    Attributes\n    ----------\n    `theta_`: array\n        Specified theta OR the best set of autocorrelation parameters (the \\\\\n        sought maximizer of the reduced likelihood function).\n\n    `reduced_likelihood_function_value_`: array\n        The optimal reduced likelihood function value.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.gaussian_process import GaussianProcess\n    >>> X = np.array([[1., 3., 5., 6., 7., 8.]]).T\n    >>> y = (X * np.sin(X)).ravel()\n    >>> gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.)\n    >>> gp.fit(X, y)                                      # doctest: +ELLIPSIS\n    GaussianProcess(beta0=None...\n            ...\n\n    Notes\n    -----\n    The presentation implementation is based on a translation of the DACE\n    Matlab toolbox, see reference [NLNS2002]_.\n\n    References\n    ----------\n\n    .. [NLNS2002] `H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.\n        Sondergaard.  DACE - A MATLAB Kriging Toolbox.` (2002)\n        http://www2.imm.dtu.dk/~hbn/dace/dace.pdf\n\n    .. [WBSWM1992] `W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,\n        and M.D.  Morris (1992). Screening, predicting, and computer\n        experiments.  Technometrics, 34(1) 15--25.`\n        http://www.jstor.org/pss/1269548\n    \"\"\"\n\n    #regression_types = {\n     #   'constant': regression.constant,\n      #  'linear': regression.linear,\n       # 'quadratic': regression.quadratic}\n\n    correlation_types = {\n        'absolute_exponential': correlation.absolute_exponential,\n        'squared_exponential': correlation.squared_exponential,\n        'generalized_exponential': correlation.generalized_exponential,\n        'cubic': correlation.cubic,\n        'linear': correlation.linear}\n\n    optimizer_types = [\n        'fmin_cobyla',\n        'Welch']\n\n    def __init__(self, regr='constant', corr='squared_exponential', beta0=None,\n                 storage_mode='full', verbose=False, theta0=1e-1,\n                 thetaL=None, thetaU=None, optimizer='fmin_cobyla',\n                 random_start=1, normalize=True,\n                 nugget=10. * MACHINE_EPSILON, random_state=None):\n\n        self.regr = regr\n        self.corr = corr\n        self.beta0 = beta0\n        self.storage_mode = storage_mode\n        self.verbose = verbose\n        self.theta0 = theta0\n        self.thetaL = thetaL\n        self.thetaU = thetaU\n        self.normalize = normalize\n        self.nugget = nugget\n        self.optimizer = optimizer\n        self.random_start = random_start\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"\n        The Gaussian Process model fitting method.\n\n        Parameters\n        ----------\n        X : double array_like\n            An array with shape (n_samples, n_features) with the input at which\n            observations were made.\n\n        y : double array_like\n            An array with shape (n_samples, ) or shape (n_samples, n_targets)\n            with the observations of the output to be predicted.\n\n        Returns\n        -------\n        gp : self\n            A fitted Gaussian Process model object awaiting data to perform\n            predictions.\n        \"\"\"\n        # Run input checks\n        self._check_params()\n\n        self.random_state = check_random_state(self.random_state)\n\n        # Force data to 2D numpy.array\n        X = array2d(X)\n        y = np.asarray(y)\n        self.y_ndim_ = y.ndim\n        if y.ndim == 1:\n            y = y[:, np.newaxis]\n        X, y = check_arrays(X, y)\n\n        # Check shapes of DOE & observations\n        n_samples, n_features = X.shape\n        n_targets = y.shape\n\n        # Run input checks\n        self._check_params(n_samples)\n\n        # Normalize data or don't\n        if self.normalize:\n            X_mean = np.mean(X, axis=0)\n            X_std = np.std(X, axis=0)\n            y_mean = np.mean(y, axis=0)\n            y_std = np.std(y, axis=0)\n            X_std[X_std == 0.] = 1.\n            y_std[y_std == 0.] = 1.\n            # center and scale X if necessary\n            X = (X - X_mean) / X_std\n            y = (y - y_mean) / y_std\n        else:\n            X_mean = np.zeros(1)\n            X_std = np.ones(1)\n            y_mean = np.zeros(1)\n            y_std = np.ones(1)\n\n        # Calculate matrix of distances D between samples\n        D, ij = l1_cross_distances(X)\n        if (np.min(np.sum(D, axis=1)) == 0.\n                and self.corr != correlation.pure_nugget):\n            raise Exception(\"Multiple input features cannot have the same\"\n                            \" target value.\")\n\n        # Regression matrix and parameters\n        F = self.regr(X)\n        n_samples_F = F.shape[0]\n        if F.ndim > 1:\n            p = F.shape[1]\n        else:\n            p = 1\n        if n_samples_F != n_samples:\n            raise Exception(\"Number of rows in F and X do not match. Most \"\n                            \"likely something is going wrong with the \"\n                            \"regression model.\")\n        if p > n_samples_F:\n            raise Exception((\"Ordinary least squares problem is undetermined \"\n                             \"n_samples=%d must be greater than the \"\n                             \"regression model size p=%d.\") % (n_samples, p))\n        if self.beta0 is not None:\n            if self.beta0.shape[0] != p:\n                raise Exception(\"Shapes of beta0 and F do not match.\")\n\n        # Set attributes\n        self.X = X\n        self.y = y\n        self.D = D\n        self.ij = ij\n        self.F = F\n        self.X_mean, self.X_std = X_mean, X_std\n        self.y_mean, self.y_std = y_mean, y_std\n\n        # Determine Gaussian Process model parameters\n        if self.thetaL is not None and self.thetaU is not None:\n            # Maximum Likelihood Estimation of the parameters\n            if self.verbose:\n                print(\"Performing Maximum Likelihood Estimation of the \"\n                      \"autocorrelation parameters...\")\n            self.theta_, self.reduced_likelihood_function_value_, par = self._arg_max_reduced_likelihood_function()\n            if np.isinf(self.reduced_likelihood_function_value_):\n                raise Exception(\"Bad parameter region. \"\n                                \"Try increasing upper bound\")\n\n\telse:\n            # Given parameters\n            if self.verbose:\n                print(\"Given autocorrelation parameters. \"\n                      \"Computing Gaussian Process model parameters...\")\n            self.theta_ = self.theta0\n            self.reduced_likelihood_function_value_, par = self.reduced_likelihood_function()\n            if np.isinf(self.reduced_likelihood_function_value_):\n                raise Exception(\"Bad point. Try increasing theta0.\")\n\n        self.beta = par['beta']\n        self.gamma = par['gamma']\n        self.sigma2 = par['sigma2']\n        self.C = par['C']\n        self.Ft = par['Ft']\n        self.G = par['G']\n\n        if self.storage_mode == 'light':\n            # Delete heavy data (it will be computed again if required)\n            # (it is required only when MSE is wanted in self.predict)\n            if self.verbose:\n                print(\"Light storage mode specified. \"\n                      \"Flushing autocorrelation matrix...\")\n            self.D = None\n            self.ij = None\n            self.F = None\n            self.C = None\n            self.Ft = None\n            self.G = None\n\n        return self\n\n    def predict(self, X, eval_MSE=False, batch_size=None):\n        \"\"\"\n        This function evaluates the Gaussian Process model at x.\n\n        Parameters\n        ----------\n        X : array_like\n            An array with shape (n_eval, n_features) giving the point(s) at\n            which the prediction(s) should be made.\n\n        eval_MSE : boolean, optional\n            A boolean specifying whether the Mean Squared Error should be\n            evaluated or not.\n            Default assumes evalMSE = False and evaluates only the BLUP (mean\n            prediction).\n\n        batch_size : integer, optional\n            An integer giving the maximum number of points that can be\n            evaluated simultaneously (depending on the available memory).\n            Default is None so that all given points are evaluated at the same\n            time.\n\n        Returns\n        -------\n        y : array_like, shape (n_samples, ) or (n_samples, n_targets)\n            An array with shape (n_eval, ) if the Gaussian Process was trained\n            on an array of shape (n_samples, ) or an array with shape\n            (n_eval, n_targets) if the Gaussian Process was trained on an array\n            of shape (n_samples, n_targets) with the Best Linear Unbiased\n            Prediction at x.\n\n        MSE : array_like, optional (if eval_MSE == True)\n            An array with shape (n_eval, ) or (n_eval, n_targets) as with y,\n            with the Mean Squared Error at x.\n        \"\"\"\n\n        # Check input shapes\n        X = array2d(X)\n        n_eval, _ = X.shape\n        n_samples, n_features = self.X.shape\n        n_samples_y, n_targets = self.y.shape\n\n        # Run input checks\n        self._check_params(n_samples)\n\n        if X.shape[1] != n_features:\n            raise ValueError((\"The number of features in X (X.shape[1] = %d) \"\n                              \"should match the number of features used \"\n                              \"for fit() \"\n                              \"which is %d.\") % (X.shape[1], n_features))\n\n        if batch_size is None:\n            # No memory management\n            # (evaluates all given points in a single batch run)\n\n            # Normalize input\n            X = (X - self.X_mean) / self.X_std\n\n            # Initialize output\n            y = np.zeros(n_eval)\n            if eval_MSE:\n                MSE = np.zeros(n_eval)\n\n            # Get pairwise componentwise L1-distances to the input training set\n            dx = manhattan_distances(X, Y=self.X, sum_over_features=False)\n            # Get regression function and correlation\n            f = self.regr(X)\n            r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)\n\n            # Scaled predictor\n            y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)\n\n            # Predictor\n            y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)\n\n            if self.y_ndim_ == 1:\n                y = y.ravel()\n\n            # Mean Squared Error\n            if eval_MSE:\n                C = self.C\n                if C is None:\n                    # Light storage mode (need to recompute C, F, Ft and G)\n                    if self.verbose:\n                        print(\"This GaussianProcess used \\'light\\' storage mode \"\n                              \"at instantiation. Need to recompute \"\n                              \"autocorrelation matrix...\")\n                    reduced_likelihood_function_value, par = self.reduced_likelihood_function()\n                    self.C = par['C']\n                    self.Ft = par['Ft']\n                    self.G = par['G']\n\n                rt = linalg.solve_triangular(self.C, r.T, lower=True)\n\n                if self.beta0 is None:\n                    # Universal Kriging\n                    u = linalg.solve_triangular(self.G.T,\n                                                np.dot(self.Ft.T, rt) - f.T)\n                else:\n                    # Ordinary Kriging\n                    u = np.zeros((n_targets, n_eval))\n\n                MSE = np.dot(self.sigma2.reshape(n_targets, 1),\n                             (1. - (rt ** 2.).sum(axis=0)\n                              + (u ** 2.).sum(axis=0))[np.newaxis, :])\n                MSE = np.sqrt((MSE ** 2.).sum(axis=0) / n_targets)\n\n                # Mean Squared Error might be slightly negative depending on\n                # machine precision: force to zero!\n                MSE[MSE < 0.] = 0.\n\n                if self.y_ndim_ == 1:\n                    MSE = MSE.ravel()\n\n                return y, MSE\n\n            else:\n\n                return y\n\n        else:\n            # Memory management\n\n            if type(batch_size) is not int or batch_size <= 0:\n                raise Exception(\"batch_size must be a positive integer\")\n\n            if eval_MSE:\n\n                y, MSE = np.zeros(n_eval), np.zeros(n_eval)\n                for k in range(max(1, n_eval / batch_size)):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])\n                    y[batch_from:batch_to], MSE[batch_from:batch_to] = self.predict(X[batch_from:batch_to],\n                                     eval_MSE=eval_MSE, batch_size=None)\n\n                return y, MSE\n\n            else:\n\n                y = np.zeros(n_eval)\n                for k in range(max(1, n_eval / batch_size)):\n                    batch_from = k * batch_size\n                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])\n                    y[batch_from:batch_to] = self.predict(X[batch_from:batch_to],\n                                     eval_MSE=eval_MSE, batch_size=None)\n\n                return y\n\n    def reduced_likelihood_function(self, theta=None):\n        \"\"\"\n        This function determines the BLUP parameters and evaluates the reduced\n        likelihood function for the given autocorrelation parameters theta.\n\n        Maximizing this function wrt the autocorrelation parameters theta is\n        equivalent to maximizing the likelihood of the assumed joint Gaussian\n        distribution of the observations y evaluated onto the design of\n        experiments X.\n\n        Parameters\n        ----------\n        theta : array_like, optional\n            An array containing the autocorrelation parameters at which the\n            Gaussian Process model parameters should be determined.\n            Default uses the built-in autocorrelation parameters\n            (ie ``theta = self.theta_``).\n\n        Returns\n        -------\n        reduced_likelihood_function_value : double\n            The value of the reduced likelihood function associated to the\n            given autocorrelation parameters theta.\n\n        par : dict\n            A dictionary containing the requested Gaussian Process model\n            parameters:\n\n                sigma2\n                        Gaussian Process variance.\n                beta\n                        Generalized least-squares regression weights for\n                        Universal Kriging or given beta0 for Ordinary\n                        Kriging.\n                gamma\n                        Gaussian Process weights.\n                C\n                        Cholesky decomposition of the correlation matrix [R].\n                Ft\n                        Solution of the linear equation system : [R] x Ft = F\n                G\n                        QR decomposition of the matrix Ft.\n        \"\"\"\n\n        if theta is None:\n            # Use built-in autocorrelation parameters\n            theta = self.theta_\n\n        # Initialize output\n        reduced_likelihood_function_value = - np.inf\n        par = {}\n\n        # Retrieve data\n        n_samples = self.X.shape[0]\n        D = self.D\n        ij = self.ij\n        F = self.F\n\n        if D is None:\n            # Light storage mode (need to recompute D, ij and F)\n            D, ij = l1_cross_distances(self.X)\n            if (np.min(np.sum(D, axis=1)) == 0.\n                    and self.corr != correlation.pure_nugget):\n                raise Exception(\"Multiple X are not allowed\")\n            F = self.regr(self.X)\n\n        # Set up R\n        r = self.corr(theta, D)\n        R = np.eye(n_samples) * (1. + self.nugget)\n        R[ij[:, 0], ij[:, 1]] = r\n        R[ij[:, 1], ij[:, 0]] = r\n\n        # Cholesky decomposition of R\n        try:\n            C = linalg.cholesky(R, lower=True)\n        except linalg.LinAlgError:\n            return reduced_likelihood_function_value, par\n\n        # Get generalized least squares solution\n        Ft = linalg.solve_triangular(C, F, lower=True)\n        try:\n            Q, G = linalg.qr(Ft, econ=True)\n        except:\n            #/usr/lib/python2.6/dist-packages/scipy/linalg/decomp.py:1177:\n            # DeprecationWarning: qr econ argument will be removed after scipy\n            # 0.7. The economy transform will then be available through the\n            # mode='economic' argument.\n            Q, G = linalg.qr(Ft, mode='economic')\n            pass\n\n        sv = linalg.svd(G, compute_uv=False)\n        rcondG = sv[-1] / sv[0]\n        if rcondG < 1e-10:\n            # Check F\n            sv = linalg.svd(F, compute_uv=False)\n            condF = sv[0] / sv[-1]\n            if condF > 1e15:\n                raise Exception(\"F is too ill conditioned. Poor combination \"\n                                \"of regression model and observations.\")\n            else:\n                # Ft is too ill conditioned, get out (try different theta)\n                return reduced_likelihood_function_value, par\n\n        Yt = linalg.solve_triangular(C, self.y, lower=True)\n        if self.beta0 is None:\n            # Universal Kriging\n            beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))\n        else:\n            # Ordinary Kriging\n            beta = np.array(self.beta0)\n\n        rho = Yt - np.dot(Ft, beta)\n        sigma2 = (rho ** 2.).sum(axis=0) / n_samples\n        # The determinant of R is equal to the squared product of the diagonal\n        # elements of its Cholesky decomposition C\n        detR = (np.diag(C) ** (2. / n_samples)).prod()\n\n        # Compute/Organize output\n        reduced_likelihood_function_value = - sigma2.sum() * detR\n        par['sigma2'] = sigma2 * self.y_std ** 2.\n        par['beta'] = beta\n        par['gamma'] = linalg.solve_triangular(C.T, rho)\n        par['C'] = C\n        par['Ft'] = Ft\n        par['G'] = G\n\n        return reduced_likelihood_function_value, par\n\n    def _arg_max_reduced_likelihood_function(self):\n        \"\"\"\n        This function estimates the autocorrelation parameters theta as the\n        maximizer of the reduced likelihood function.\n        (Minimization of the opposite reduced likelihood function is used for\n        convenience)\n\n        Parameters\n        ----------\n        self : All parameters are stored in the Gaussian Process model object.\n\n        Returns\n        -------\n        optimal_theta : array_like\n            The best set of autocorrelation parameters (the sought maximizer of\n            the reduced likelihood function).\n\n        optimal_reduced_likelihood_function_value : double\n            The optimal reduced likelihood function value.\n\n        optimal_par : dict\n            The BLUP parameters associated to thetaOpt.\n        \"\"\"\n\n        # Initialize output\n        best_optimal_theta = []\n        best_optimal_rlf_value = []\n        best_optimal_par = []\n\n        if self.verbose:\n            print(\"The chosen optimizer is: \" + str(self.optimizer))\n            if self.random_start > 1:\n                print(str(self.random_start) + \" random starts are required.\")\n\n        percent_completed = 0.\n\n        # Force optimizer to fmin_cobyla if the model is meant to be isotropic\n        if self.optimizer == 'Welch' and self.theta0.size == 1:\n            self.optimizer = 'fmin_cobyla'\n\n        if self.optimizer == 'fmin_cobyla':\n\n            def minus_reduced_likelihood_function(log10t):\n                return - self.reduced_likelihood_function(\n                    theta=10. ** log10t)[0]\n\n            constraints = []\n            for i in range(self.theta0.size):\n                constraints.append(lambda log10t, i=i:\n                                   log10t[i] - np.log10(self.thetaL[0, i]))\n                constraints.append(lambda log10t, i=i:\n                                   np.log10(self.thetaU[0, i]) - log10t[i])\n\n            for k in range(self.random_start):\n\n                if k == 0:\n                    # Use specified starting point as first guess\n                    theta0 = self.theta0\n                else:\n                    # Generate a random starting point log10-uniformly\n                    # distributed between bounds\n                    log10theta0 = np.log10(self.thetaL)+ self.random_state.rand(self.theta0.size).reshape(\n                            self.theta0.shape) * np.log10(self.thetaU\n                                                          / self.thetaL)\n                    theta0 = 10. ** log10theta0\n\n                # Run Cobyla\n                try:\n                    log10_optimal_theta =  optimize.fmin_cobyla(minus_reduced_likelihood_function,\n                                             np.log10(theta0), constraints,\n                                             iprint=0)\n                except ValueError as ve:\n                    print(\"Optimization failed. Try increasing the ``nugget``\")\n                    raise ve\n\n                optimal_theta = 10. ** log10_optimal_theta\n                optimal_rlf_value, optimal_par =  self.reduced_likelihood_function(theta=optimal_theta)\n\n                # Compare the new optimizer to the best previous one\n                if k > 0:\n                    if optimal_rlf_value > best_optimal_rlf_value:\n                        best_optimal_rlf_value = optimal_rlf_value\n                        best_optimal_par = optimal_par\n                        best_optimal_theta = optimal_theta\n                else:\n                    best_optimal_rlf_value = optimal_rlf_value\n                    best_optimal_par = optimal_par\n                    best_optimal_theta = optimal_theta\n                if self.verbose and self.random_start > 1:\n                    if (20 * k) / self.random_start > percent_completed:\n                        percent_completed = (20 * k) / self.random_start\n                        print(\"%s completed\" % (5 * percent_completed))\n\n            optimal_rlf_value = best_optimal_rlf_value\n            optimal_par = best_optimal_par\n            optimal_theta = best_optimal_theta\n\n        elif self.optimizer == 'Welch':\n\n            # Backup of the given atrributes\n            theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU\n            corr = self.corr\n            verbose = self.verbose\n\n            # This will iterate over fmin_cobyla optimizer\n            self.optimizer = 'fmin_cobyla'\n            self.verbose = False\n\n            # Initialize under isotropy assumption\n            if verbose:\n                print(\"Initialize under isotropy assumption...\")\n            self.theta0 = array2d(self.theta0.min())\n            self.thetaL = array2d(self.thetaL.min())\n            self.thetaU = array2d(self.thetaU.max())\n            theta_iso, optimal_rlf_value_iso, par_iso =  self._arg_max_reduced_likelihood_function()\n            optimal_theta = theta_iso + np.zeros(theta0.shape)\n\n            # Iterate over all dimensions of theta allowing for anisotropy\n            if verbose:\n                print(\"Now improving allowing for anisotropy...\")\n            for i in self.random_state.permutation(theta0.size):\n                if verbose:\n                    print(\"Proceeding along dimension %d...\" % (i + 1))\n                self.theta0 = array2d(theta_iso)\n                self.thetaL = array2d(thetaL[0, i])\n                self.thetaU = array2d(thetaU[0, i])\n\n                def corr_cut(t, d):\n                    return corr(array2d(np.hstack([optimal_theta[0][0:i],\n                                                   t[0],\n                                                   optimal_theta[0][(i + 1)::]]\n                                                  )), d)\n\n                self.corr = corr_cut\n                optimal_theta[0, i], optimal_rlf_value, optimal_par = self._arg_max_reduced_likelihood_function()\n\n            # Restore the given atrributes\n            self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU\n            self.corr = corr\n            self.optimizer = 'Welch'\n            self.verbose = verbose\n\n        else:\n\n            raise NotImplementedError(\"This optimizer (\\'%s\\') is not \"\n                                      \"implemented yet. Please contribute!\"\n                                      % self.optimizer)\n\n        return optimal_theta, optimal_rlf_value, optimal_par\n\n    def _check_params(self, n_samples=None):\n\n        # Check regression model\n        if not callable(self.regr):\n            if self.regr in self._regression_types:\n                self.regr = self._regression_types[self.regr]\n            else:\n                raise ValueError(\"regr should be one of %s or callable, \"\n                                 \"%s was given.\"\n                                 % (self._regression_types.keys(), self.regr))\n\n        # Check regression weights if given (Ordinary Kriging)\n        if self.beta0 is not None:\n            self.beta0 = array2d(self.beta0)\n            if self.beta0.shape[1] != 1:\n                # Force to column vector\n                self.beta0 = self.beta0.T\n\n        # Check correlation model\n        if not callable(self.corr):\n            if self.corr in self._correlation_types:\n                self.corr = self._correlation_types[self.corr]\n            else:\n                raise ValueError(\"corr should be one of %s or callable, \"\n                                 \"%s was given.\"\n                                 % (self._correlation_types.keys(), self.corr))\n\n        # Check storage mode\n        if self.storage_mode != 'full' and self.storage_mode != 'light':\n            raise ValueError(\"Storage mode should either be \\'full\\' or \"\n                             \"\\'light\\', %s was given.\" % self.storage_mode)\n\n        # Check correlation parameters\n        self.theta0 = array2d(self.theta0)\n        lth = self.theta0.size\n\n        if self.thetaL is not None and self.thetaU is not None:\n            self.thetaL = array2d(self.thetaL)\n            self.thetaU = array2d(self.thetaU)\n            if self.thetaL.size != lth or self.thetaU.size != lth:\n                raise ValueError(\"theta0, thetaL and thetaU must have the \"\n                                 \"same length.\")\n            if np.any(self.thetaL <= 0) or np.any(self.thetaU < self.thetaL):\n                raise ValueError(\"The bounds must satisfy O < thetaL <= \"\n                                 \"thetaU.\")\n\n        elif self.thetaL is None and self.thetaU is None:\n            if np.any(self.theta0 <= 0):\n                raise ValueError(\"theta0 must be strictly positive.\")\n\n        elif self.thetaL is None or self.thetaU is None:\n            raise ValueError(\"thetaL and thetaU should either be both or \"\n                             \"neither specified.\")\n\n        # Force verbose type to bool\n        self.verbose = bool(self.verbose)\n\n        # Force normalize type to bool\n        self.normalize = bool(self.normalize)\n\n        # Check nugget value\n        self.nugget = np.asarray(self.nugget)\n        if np.any(self.nugget) < 0.:\n            raise ValueError(\"nugget must be positive or zero.\")\n        if (n_samples is not None\n                and self.nugget.shape not in [(), (n_samples,)]):\n            raise ValueError(\"nugget must be either a scalar \"\n                             \"or array of length n_samples.\")\n\n        # Check optimizer\n        if not self.optimizer in self._optimizer_types:\n            raise ValueError(\"optimizer should be one of %s\"\n                             % self._optimizer_types)\n\n        # Force random_start type to int\n        self.random_start = int(self.random_start)\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'correlation' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-22-4bde7b70259b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGaussianProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"The Gaussian Process model class.\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-22-4bde7b70259b>\u001b[0m in \u001b[0;36mGaussianProcess\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     correlation_types = {\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;34m'absolute_exponential'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute_exponential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;34m'squared_exponential'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquared_exponential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;34m'generalized_exponential'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneralized_exponential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'correlation' is not defined"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "y =g(X)\ny.shape",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "gp.fit(X,y)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "res = 50\nx1,x2 = np.meshgrid(np.linspace(-lim,lim,res),\n                     np.linspace(-lim,lim,res))",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "xx = np.vstack([x1.reshape(x1.size),x2.reshape(x2.size)]).T",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "y_true = g(xx)\ny_pred, MSE = gp.predict(xx,eval_MSE=True)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sigma = np.sqrt(MSE)\ny_true = y_true.reshape((res,res))\ny_pred = y_pred.reshape((res,res))\nsigma = y_pred.reshape((res,res))\nk = PHIinv (.975)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "fig = plt.figure(1)\nax = fig.add_subplot(111)\nax.axes.set_aspect('equal')\nplt.xticks([])\nplt.yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.xlabel('$x_1\nplt.ylabel('$x_2$')\ncax = plt.imshow(np.flipud(PHI(- y_pred / sigma)), cmap=cm.gray_r, alpha=0.8,\n                extent=(- lim, lim, - lim, lim))\nnorm = plt.matplotlib.colors.Normalize(vmin= 0.,vmax=0.9)\ncb = plt.colorbar(cax,ticks = [0., 0.2, 0.4, 0.6, 0.8, 1.],norm= norm)\ncb.set_label('${\\\\rm \\mathbb{P}}\\left[\\widehat{G}(\\mathbf{x}) \\leq 0\\\\right]$')\nplt.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)\nplt.plot(X[y> 0, 0], X[y > 0, 1], 'b.', markersize=12)\ncs = plt.contour(x1,x2,y_true,[0.],colors='r',linestyles = 'dashdot')\ncs = plt.contour(x1,x2,PHI(-y_pred/sigma),[0.025],colors='b',linestyles ='solid')\nplt.clabel(cs,fontsize =11)\ncs = plt.contour(x1,x2,PHI(-y_pred/sigma),[0.5],colors='g',linestyles ='dashed')\nplt.clabel(cs,fontsize =11)\ncs =plt.contour(x1,x2,PHI(-y_pred/sigma),[0.975],colors='r',linestyles ='solid')\nplt.clabel(cs,fontsize =11)\nplt.show()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "plt.contour \u662f\u8f6e\u5ed3\uff0c\u6709\u4ec0\u4e48\u610f\u4e49\uff1f"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}