{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# gaussian process"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import gaussian_process"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f(x):\n",
      "    return x* np.sin(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.atleast_2d(np.linspace(-10,10,50)).T\n",
      "y = f(X).ravel()\n",
      "x = np.atleast_2d(np.linspace(-10, 10, 15)).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u8fd9\u91ccX\u6570\u636e\u4e0d\u80fd\u8d85\u8fc7100\uff1f"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X.shape\n",
      "print y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50, 1)\n",
        "(50,)\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gp = gaussian_process.GaussianProcess(theta0 = 1e-2, thetaL = 1e-4, thetaU =1e-1)\n",
      "gp.fit(X,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "GaussianProcess(beta0=None,\n",
        "        corr=<function squared_exponential at 0x7fb0991315f0>,\n",
        "        normalize=True, nugget=array(2.220446049250313e-15),\n",
        "        optimizer='fmin_cobyla', random_start=1,\n",
        "        random_state=<mtrand.RandomState object at 0x7fb0c54dc810>,\n",
        "        regr=<function constant at 0x7fb099131230>, storage_mode='full',\n",
        "        theta0=array([[ 0.01]]), thetaL=array([[ 0.0001]]),\n",
        "        thetaU=array([[ 0.1]]), verbose=False)"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import inspect\n",
      "\n",
      "inspect.getfile(gaussian_process)\n",
      "inspect.getsourcelines(gaussian_process.GaussianProcess)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "(['class GaussianProcess(BaseEstimator, RegressorMixin):\\n',\n",
        "  '    \"\"\"The Gaussian Process model class.\\n',\n",
        "  '\\n',\n",
        "  '    Parameters\\n',\n",
        "  '    ----------\\n',\n",
        "  '    regr : string or callable, optional\\n',\n",
        "  '        A regression function returning an array of outputs of the linear\\n',\n",
        "  '        regression functional basis. The number of observations n_samples\\n',\n",
        "  '        should be greater than the size p of this basis.\\n',\n",
        "  '        Default assumes a simple constant regression trend.\\n',\n",
        "  '        Available built-in regression models are::\\n',\n",
        "  '\\n',\n",
        "  \"            'constant', 'linear', 'quadratic'\\n\",\n",
        "  '\\n',\n",
        "  '    corr : string or callable, optional\\n',\n",
        "  '        A stationary autocorrelation function returning the autocorrelation\\n',\n",
        "  \"        between two points x and x'.\\n\",\n",
        "  '        Default assumes a squared-exponential autocorrelation model.\\n',\n",
        "  '        Built-in correlation models are::\\n',\n",
        "  '\\n',\n",
        "  \"            'absolute_exponential', 'squared_exponential',\\n\",\n",
        "  \"            'generalized_exponential', 'cubic', 'linear'\\n\",\n",
        "  '\\n',\n",
        "  '    beta0 : double array_like, optional\\n',\n",
        "  '        The regression weight vector to perform Ordinary Kriging (OK).\\n',\n",
        "  '        Default assumes Universal Kriging (UK) so that the vector beta of\\n',\n",
        "  '        regression weights is estimated using the maximum likelihood\\n',\n",
        "  '        principle.\\n',\n",
        "  '\\n',\n",
        "  '    storage_mode : string, optional\\n',\n",
        "  '        A string specifying whether the Cholesky decomposition of the\\n',\n",
        "  '        correlation matrix should be stored in the class (storage_mode =\\n',\n",
        "  \"        'full') or not (storage_mode = 'light').\\n\",\n",
        "  \"        Default assumes storage_mode = 'full', so that the\\n\",\n",
        "  '        Cholesky decomposition of the correlation matrix is stored.\\n',\n",
        "  '        This might be a useful parameter when one is not interested in the\\n',\n",
        "  '        MSE and only plan to estimate the BLUP, for which the correlation\\n',\n",
        "  '        matrix is not required.\\n',\n",
        "  '\\n',\n",
        "  '    verbose : boolean, optional\\n',\n",
        "  '        A boolean specifying the verbose level.\\n',\n",
        "  '        Default is verbose = False.\\n',\n",
        "  '\\n',\n",
        "  '    theta0 : double array_like, optional\\n',\n",
        "  '        An array with shape (n_features, ) or (1, ).\\n',\n",
        "  '        The parameters in the autocorrelation model.\\n',\n",
        "  '        If thetaL and thetaU are also specified, theta0 is considered as\\n',\n",
        "  '        the starting point for the maximum likelihood estimation of the\\n',\n",
        "  '        best set of parameters.\\n',\n",
        "  '        Default assumes isotropic autocorrelation model with theta0 = 1e-1.\\n',\n",
        "  '\\n',\n",
        "  '    thetaL : double array_like, optional\\n',\n",
        "  \"        An array with shape matching theta0's.\\n\",\n",
        "  '        Lower bound on the autocorrelation parameters for maximum\\n',\n",
        "  '        likelihood estimation.\\n',\n",
        "  '        Default is None, so that it skips maximum likelihood estimation and\\n',\n",
        "  '        it uses theta0.\\n',\n",
        "  '\\n',\n",
        "  '    thetaU : double array_like, optional\\n',\n",
        "  \"        An array with shape matching theta0's.\\n\",\n",
        "  '        Upper bound on the autocorrelation parameters for maximum\\n',\n",
        "  '        likelihood estimation.\\n',\n",
        "  '        Default is None, so that it skips maximum likelihood estimation and\\n',\n",
        "  '        it uses theta0.\\n',\n",
        "  '\\n',\n",
        "  '    normalize : boolean, optional\\n',\n",
        "  '        Input X and observations y are centered and reduced wrt\\n',\n",
        "  '        means and standard deviations estimated from the n_samples\\n',\n",
        "  '        observations provided.\\n',\n",
        "  '        Default is normalize = True so that data is normalized to ease\\n',\n",
        "  '        maximum likelihood estimation.\\n',\n",
        "  '\\n',\n",
        "  '    nugget : double or ndarray, optional\\n',\n",
        "  '        Introduce a nugget effect to allow smooth predictions from noisy\\n',\n",
        "  '        data.  If nugget is an ndarray, it must be the same length as the\\n',\n",
        "  '        number of data points used for the fit.\\n',\n",
        "  '        The nugget is added to the diagonal of the assumed training covariance;\\n',\n",
        "  '        in this way it acts as a Tikhonov regularization in the problem.  In\\n',\n",
        "  '        the special case of the squared exponential correlation function, the\\n',\n",
        "  '        nugget mathematically represents the variance of the input values.\\n',\n",
        "  '        Default assumes a nugget close to machine precision for the sake of\\n',\n",
        "  '        robustness (nugget = 10. * MACHINE_EPSILON).\\n',\n",
        "  '\\n',\n",
        "  '    optimizer : string, optional\\n',\n",
        "  '        A string specifying the optimization algorithm to be used.\\n',\n",
        "  \"        Default uses 'fmin_cobyla' algorithm from scipy.optimize.\\n\",\n",
        "  '        Available optimizers are::\\n',\n",
        "  '\\n',\n",
        "  \"            'fmin_cobyla', 'Welch'\\n\",\n",
        "  '\\n',\n",
        "  \"        'Welch' optimizer is dued to Welch et al., see reference [WBSWM1992]_.\\n\",\n",
        "  '        It consists in iterating over several one-dimensional optimizations\\n',\n",
        "  '        instead of running one single multi-dimensional optimization.\\n',\n",
        "  '\\n',\n",
        "  '    random_start : int, optional\\n',\n",
        "  '        The number of times the Maximum Likelihood Estimation should be\\n',\n",
        "  '        performed from a random starting point.\\n',\n",
        "  '        The first MLE always uses the specified starting point (theta0),\\n',\n",
        "  '        the next starting points are picked at random according to an\\n',\n",
        "  '        exponential distribution (log-uniform on [thetaL, thetaU]).\\n',\n",
        "  '        Default does not use random starting point (random_start = 1).\\n',\n",
        "  '\\n',\n",
        "  '    random_state: integer or numpy.RandomState, optional\\n',\n",
        "  '        The generator used to shuffle the sequence of coordinates of theta in\\n',\n",
        "  '        the Welch optimizer. If an integer is given, it fixes the seed.\\n',\n",
        "  '        Defaults to the global numpy random number generator.\\n',\n",
        "  '\\n',\n",
        "  '\\n',\n",
        "  '    Attributes\\n',\n",
        "  '    ----------\\n',\n",
        "  '    `theta_`: array\\n',\n",
        "  '        Specified theta OR the best set of autocorrelation parameters (the \\\\\\n',\n",
        "  '        sought maximizer of the reduced likelihood function).\\n',\n",
        "  '\\n',\n",
        "  '    `reduced_likelihood_function_value_`: array\\n',\n",
        "  '        The optimal reduced likelihood function value.\\n',\n",
        "  '\\n',\n",
        "  '    Examples\\n',\n",
        "  '    --------\\n',\n",
        "  '    >>> import numpy as np\\n',\n",
        "  '    >>> from sklearn.gaussian_process import GaussianProcess\\n',\n",
        "  '    >>> X = np.array([[1., 3., 5., 6., 7., 8.]]).T\\n',\n",
        "  '    >>> y = (X * np.sin(X)).ravel()\\n',\n",
        "  '    >>> gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.)\\n',\n",
        "  '    >>> gp.fit(X, y)                                      # doctest: +ELLIPSIS\\n',\n",
        "  '    GaussianProcess(beta0=None...\\n',\n",
        "  '            ...\\n',\n",
        "  '\\n',\n",
        "  '    Notes\\n',\n",
        "  '    -----\\n',\n",
        "  '    The presentation implementation is based on a translation of the DACE\\n',\n",
        "  '    Matlab toolbox, see reference [NLNS2002]_.\\n',\n",
        "  '\\n',\n",
        "  '    References\\n',\n",
        "  '    ----------\\n',\n",
        "  '\\n',\n",
        "  '    .. [NLNS2002] `H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.\\n',\n",
        "  '        Sondergaard.  DACE - A MATLAB Kriging Toolbox.` (2002)\\n',\n",
        "  '        http://www2.imm.dtu.dk/~hbn/dace/dace.pdf\\n',\n",
        "  '\\n',\n",
        "  '    .. [WBSWM1992] `W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,\\n',\n",
        "  '        and M.D.  Morris (1992). Screening, predicting, and computer\\n',\n",
        "  '        experiments.  Technometrics, 34(1) 15--25.`\\n',\n",
        "  '        http://www.jstor.org/pss/1269548\\n',\n",
        "  '    \"\"\"\\n',\n",
        "  '\\n',\n",
        "  '    _regression_types = {\\n',\n",
        "  \"        'constant': regression.constant,\\n\",\n",
        "  \"        'linear': regression.linear,\\n\",\n",
        "  \"        'quadratic': regression.quadratic}\\n\",\n",
        "  '\\n',\n",
        "  '    _correlation_types = {\\n',\n",
        "  \"        'absolute_exponential': correlation.absolute_exponential,\\n\",\n",
        "  \"        'squared_exponential': correlation.squared_exponential,\\n\",\n",
        "  \"        'generalized_exponential': correlation.generalized_exponential,\\n\",\n",
        "  \"        'cubic': correlation.cubic,\\n\",\n",
        "  \"        'linear': correlation.linear}\\n\",\n",
        "  '\\n',\n",
        "  '    _optimizer_types = [\\n',\n",
        "  \"        'fmin_cobyla',\\n\",\n",
        "  \"        'Welch']\\n\",\n",
        "  '\\n',\n",
        "  \"    def __init__(self, regr='constant', corr='squared_exponential', beta0=None,\\n\",\n",
        "  \"                 storage_mode='full', verbose=False, theta0=1e-1,\\n\",\n",
        "  \"                 thetaL=None, thetaU=None, optimizer='fmin_cobyla',\\n\",\n",
        "  '                 random_start=1, normalize=True,\\n',\n",
        "  '                 nugget=10. * MACHINE_EPSILON, random_state=None):\\n',\n",
        "  '\\n',\n",
        "  '        self.regr = regr\\n',\n",
        "  '        self.corr = corr\\n',\n",
        "  '        self.beta0 = beta0\\n',\n",
        "  '        self.storage_mode = storage_mode\\n',\n",
        "  '        self.verbose = verbose\\n',\n",
        "  '        self.theta0 = theta0\\n',\n",
        "  '        self.thetaL = thetaL\\n',\n",
        "  '        self.thetaU = thetaU\\n',\n",
        "  '        self.normalize = normalize\\n',\n",
        "  '        self.nugget = nugget\\n',\n",
        "  '        self.optimizer = optimizer\\n',\n",
        "  '        self.random_start = random_start\\n',\n",
        "  '        self.random_state = random_state\\n',\n",
        "  '\\n',\n",
        "  '    def fit(self, X, y):\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '        The Gaussian Process model fitting method.\\n',\n",
        "  '\\n',\n",
        "  '        Parameters\\n',\n",
        "  '        ----------\\n',\n",
        "  '        X : double array_like\\n',\n",
        "  '            An array with shape (n_samples, n_features) with the input at which\\n',\n",
        "  '            observations were made.\\n',\n",
        "  '\\n',\n",
        "  '        y : double array_like\\n',\n",
        "  '            An array with shape (n_samples, ) or shape (n_samples, n_targets)\\n',\n",
        "  '            with the observations of the output to be predicted.\\n',\n",
        "  '\\n',\n",
        "  '        Returns\\n',\n",
        "  '        -------\\n',\n",
        "  '        gp : self\\n',\n",
        "  '            A fitted Gaussian Process model object awaiting data to perform\\n',\n",
        "  '            predictions.\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '        # Run input checks\\n',\n",
        "  '        self._check_params()\\n',\n",
        "  '\\n',\n",
        "  '        self.random_state = check_random_state(self.random_state)\\n',\n",
        "  '\\n',\n",
        "  '        # Force data to 2D numpy.array\\n',\n",
        "  '        X = array2d(X)\\n',\n",
        "  '        y = np.asarray(y)\\n',\n",
        "  '        self.y_ndim_ = y.ndim\\n',\n",
        "  '        if y.ndim == 1:\\n',\n",
        "  '            y = y[:, np.newaxis]\\n',\n",
        "  '        X, y = check_arrays(X, y)\\n',\n",
        "  '\\n',\n",
        "  '        # Check shapes of DOE & observations\\n',\n",
        "  '        n_samples, n_features = X.shape\\n',\n",
        "  '        _, n_targets = y.shape\\n',\n",
        "  '\\n',\n",
        "  '        # Run input checks\\n',\n",
        "  '        self._check_params(n_samples)\\n',\n",
        "  '\\n',\n",
        "  \"        # Normalize data or don't\\n\",\n",
        "  '        if self.normalize:\\n',\n",
        "  '            X_mean = np.mean(X, axis=0)\\n',\n",
        "  '            X_std = np.std(X, axis=0)\\n',\n",
        "  '            y_mean = np.mean(y, axis=0)\\n',\n",
        "  '            y_std = np.std(y, axis=0)\\n',\n",
        "  '            X_std[X_std == 0.] = 1.\\n',\n",
        "  '            y_std[y_std == 0.] = 1.\\n',\n",
        "  '            # center and scale X if necessary\\n',\n",
        "  '            X = (X - X_mean) / X_std\\n',\n",
        "  '            y = (y - y_mean) / y_std\\n',\n",
        "  '        else:\\n',\n",
        "  '            X_mean = np.zeros(1)\\n',\n",
        "  '            X_std = np.ones(1)\\n',\n",
        "  '            y_mean = np.zeros(1)\\n',\n",
        "  '            y_std = np.ones(1)\\n',\n",
        "  '\\n',\n",
        "  '        # Calculate matrix of distances D between samples\\n',\n",
        "  '        D, ij = l1_cross_distances(X)\\n',\n",
        "  '        if (np.min(np.sum(D, axis=1)) == 0.\\n',\n",
        "  '                and self.corr != correlation.pure_nugget):\\n',\n",
        "  '            raise Exception(\"Multiple input features cannot have the same\"\\n',\n",
        "  '                            \" target value.\")\\n',\n",
        "  '\\n',\n",
        "  '        # Regression matrix and parameters\\n',\n",
        "  '        F = self.regr(X)\\n',\n",
        "  '        n_samples_F = F.shape[0]\\n',\n",
        "  '        if F.ndim > 1:\\n',\n",
        "  '            p = F.shape[1]\\n',\n",
        "  '        else:\\n',\n",
        "  '            p = 1\\n',\n",
        "  '        if n_samples_F != n_samples:\\n',\n",
        "  '            raise Exception(\"Number of rows in F and X do not match. Most \"\\n',\n",
        "  '                            \"likely something is going wrong with the \"\\n',\n",
        "  '                            \"regression model.\")\\n',\n",
        "  '        if p > n_samples_F:\\n',\n",
        "  '            raise Exception((\"Ordinary least squares problem is undetermined \"\\n',\n",
        "  '                             \"n_samples=%d must be greater than the \"\\n',\n",
        "  '                             \"regression model size p=%d.\") % (n_samples, p))\\n',\n",
        "  '        if self.beta0 is not None:\\n',\n",
        "  '            if self.beta0.shape[0] != p:\\n',\n",
        "  '                raise Exception(\"Shapes of beta0 and F do not match.\")\\n',\n",
        "  '\\n',\n",
        "  '        # Set attributes\\n',\n",
        "  '        self.X = X\\n',\n",
        "  '        self.y = y\\n',\n",
        "  '        self.D = D\\n',\n",
        "  '        self.ij = ij\\n',\n",
        "  '        self.F = F\\n',\n",
        "  '        self.X_mean, self.X_std = X_mean, X_std\\n',\n",
        "  '        self.y_mean, self.y_std = y_mean, y_std\\n',\n",
        "  '\\n',\n",
        "  '        # Determine Gaussian Process model parameters\\n',\n",
        "  '        if self.thetaL is not None and self.thetaU is not None:\\n',\n",
        "  '            # Maximum Likelihood Estimation of the parameters\\n',\n",
        "  '            if self.verbose:\\n',\n",
        "  '                print(\"Performing Maximum Likelihood Estimation of the \"\\n',\n",
        "  '                      \"autocorrelation parameters...\")\\n',\n",
        "  '            self.theta_, self.reduced_likelihood_function_value_, par = \\\\\\n',\n",
        "  '                self._arg_max_reduced_likelihood_function()\\n',\n",
        "  '            if np.isinf(self.reduced_likelihood_function_value_):\\n',\n",
        "  '                raise Exception(\"Bad parameter region. \"\\n',\n",
        "  '                                \"Try increasing upper bound\")\\n',\n",
        "  '\\n',\n",
        "  '        else:\\n',\n",
        "  '            # Given parameters\\n',\n",
        "  '            if self.verbose:\\n',\n",
        "  '                print(\"Given autocorrelation parameters. \"\\n',\n",
        "  '                      \"Computing Gaussian Process model parameters...\")\\n',\n",
        "  '            self.theta_ = self.theta0\\n',\n",
        "  '            self.reduced_likelihood_function_value_, par = \\\\\\n',\n",
        "  '                self.reduced_likelihood_function()\\n',\n",
        "  '            if np.isinf(self.reduced_likelihood_function_value_):\\n',\n",
        "  '                raise Exception(\"Bad point. Try increasing theta0.\")\\n',\n",
        "  '\\n',\n",
        "  \"        self.beta = par['beta']\\n\",\n",
        "  \"        self.gamma = par['gamma']\\n\",\n",
        "  \"        self.sigma2 = par['sigma2']\\n\",\n",
        "  \"        self.C = par['C']\\n\",\n",
        "  \"        self.Ft = par['Ft']\\n\",\n",
        "  \"        self.G = par['G']\\n\",\n",
        "  '\\n',\n",
        "  \"        if self.storage_mode == 'light':\\n\",\n",
        "  '            # Delete heavy data (it will be computed again if required)\\n',\n",
        "  '            # (it is required only when MSE is wanted in self.predict)\\n',\n",
        "  '            if self.verbose:\\n',\n",
        "  '                print(\"Light storage mode specified. \"\\n',\n",
        "  '                      \"Flushing autocorrelation matrix...\")\\n',\n",
        "  '            self.D = None\\n',\n",
        "  '            self.ij = None\\n',\n",
        "  '            self.F = None\\n',\n",
        "  '            self.C = None\\n',\n",
        "  '            self.Ft = None\\n',\n",
        "  '            self.G = None\\n',\n",
        "  '\\n',\n",
        "  '        return self\\n',\n",
        "  '\\n',\n",
        "  '    def predict(self, X, eval_MSE=False, batch_size=None):\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '        This function evaluates the Gaussian Process model at x.\\n',\n",
        "  '\\n',\n",
        "  '        Parameters\\n',\n",
        "  '        ----------\\n',\n",
        "  '        X : array_like\\n',\n",
        "  '            An array with shape (n_eval, n_features) giving the point(s) at\\n',\n",
        "  '            which the prediction(s) should be made.\\n',\n",
        "  '\\n',\n",
        "  '        eval_MSE : boolean, optional\\n',\n",
        "  '            A boolean specifying whether the Mean Squared Error should be\\n',\n",
        "  '            evaluated or not.\\n',\n",
        "  '            Default assumes evalMSE = False and evaluates only the BLUP (mean\\n',\n",
        "  '            prediction).\\n',\n",
        "  '\\n',\n",
        "  '        batch_size : integer, optional\\n',\n",
        "  '            An integer giving the maximum number of points that can be\\n',\n",
        "  '            evaluated simultaneously (depending on the available memory).\\n',\n",
        "  '            Default is None so that all given points are evaluated at the same\\n',\n",
        "  '            time.\\n',\n",
        "  '\\n',\n",
        "  '        Returns\\n',\n",
        "  '        -------\\n',\n",
        "  '        y : array_like, shape (n_samples, ) or (n_samples, n_targets)\\n',\n",
        "  '            An array with shape (n_eval, ) if the Gaussian Process was trained\\n',\n",
        "  '            on an array of shape (n_samples, ) or an array with shape\\n',\n",
        "  '            (n_eval, n_targets) if the Gaussian Process was trained on an array\\n',\n",
        "  '            of shape (n_samples, n_targets) with the Best Linear Unbiased\\n',\n",
        "  '            Prediction at x.\\n',\n",
        "  '\\n',\n",
        "  '        MSE : array_like, optional (if eval_MSE == True)\\n',\n",
        "  '            An array with shape (n_eval, ) or (n_eval, n_targets) as with y,\\n',\n",
        "  '            with the Mean Squared Error at x.\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '\\n',\n",
        "  '        # Check input shapes\\n',\n",
        "  '        X = array2d(X)\\n',\n",
        "  '        n_eval, _ = X.shape\\n',\n",
        "  '        n_samples, n_features = self.X.shape\\n',\n",
        "  '        n_samples_y, n_targets = self.y.shape\\n',\n",
        "  '\\n',\n",
        "  '        # Run input checks\\n',\n",
        "  '        self._check_params(n_samples)\\n',\n",
        "  '\\n',\n",
        "  '        if X.shape[1] != n_features:\\n',\n",
        "  '            raise ValueError((\"The number of features in X (X.shape[1] = %d) \"\\n',\n",
        "  '                              \"should match the number of features used \"\\n',\n",
        "  '                              \"for fit() \"\\n',\n",
        "  '                              \"which is %d.\") % (X.shape[1], n_features))\\n',\n",
        "  '\\n',\n",
        "  '        if batch_size is None:\\n',\n",
        "  '            # No memory management\\n',\n",
        "  '            # (evaluates all given points in a single batch run)\\n',\n",
        "  '\\n',\n",
        "  '            # Normalize input\\n',\n",
        "  '            X = (X - self.X_mean) / self.X_std\\n',\n",
        "  '\\n',\n",
        "  '            # Initialize output\\n',\n",
        "  '            y = np.zeros(n_eval)\\n',\n",
        "  '            if eval_MSE:\\n',\n",
        "  '                MSE = np.zeros(n_eval)\\n',\n",
        "  '\\n',\n",
        "  '            # Get pairwise componentwise L1-distances to the input training set\\n',\n",
        "  '            dx = manhattan_distances(X, Y=self.X, sum_over_features=False)\\n',\n",
        "  '            # Get regression function and correlation\\n',\n",
        "  '            f = self.regr(X)\\n',\n",
        "  '            r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)\\n',\n",
        "  '\\n',\n",
        "  '            # Scaled predictor\\n',\n",
        "  '            y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)\\n',\n",
        "  '\\n',\n",
        "  '            # Predictor\\n',\n",
        "  '            y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)\\n',\n",
        "  '\\n',\n",
        "  '            if self.y_ndim_ == 1:\\n',\n",
        "  '                y = y.ravel()\\n',\n",
        "  '\\n',\n",
        "  '            # Mean Squared Error\\n',\n",
        "  '            if eval_MSE:\\n',\n",
        "  '                C = self.C\\n',\n",
        "  '                if C is None:\\n',\n",
        "  '                    # Light storage mode (need to recompute C, F, Ft and G)\\n',\n",
        "  '                    if self.verbose:\\n',\n",
        "  '                        print(\"This GaussianProcess used \\'light\\' storage mode \"\\n',\n",
        "  '                              \"at instantiation. Need to recompute \"\\n',\n",
        "  '                              \"autocorrelation matrix...\")\\n',\n",
        "  '                    reduced_likelihood_function_value, par = \\\\\\n',\n",
        "  '                        self.reduced_likelihood_function()\\n',\n",
        "  \"                    self.C = par['C']\\n\",\n",
        "  \"                    self.Ft = par['Ft']\\n\",\n",
        "  \"                    self.G = par['G']\\n\",\n",
        "  '\\n',\n",
        "  '                rt = linalg.solve_triangular(self.C, r.T, lower=True)\\n',\n",
        "  '\\n',\n",
        "  '                if self.beta0 is None:\\n',\n",
        "  '                    # Universal Kriging\\n',\n",
        "  '                    u = linalg.solve_triangular(self.G.T,\\n',\n",
        "  '                                                np.dot(self.Ft.T, rt) - f.T)\\n',\n",
        "  '                else:\\n',\n",
        "  '                    # Ordinary Kriging\\n',\n",
        "  '                    u = np.zeros((n_targets, n_eval))\\n',\n",
        "  '\\n',\n",
        "  '                MSE = np.dot(self.sigma2.reshape(n_targets, 1),\\n',\n",
        "  '                             (1. - (rt ** 2.).sum(axis=0)\\n',\n",
        "  '                              + (u ** 2.).sum(axis=0))[np.newaxis, :])\\n',\n",
        "  '                MSE = np.sqrt((MSE ** 2.).sum(axis=0) / n_targets)\\n',\n",
        "  '\\n',\n",
        "  '                # Mean Squared Error might be slightly negative depending on\\n',\n",
        "  '                # machine precision: force to zero!\\n',\n",
        "  '                MSE[MSE < 0.] = 0.\\n',\n",
        "  '\\n',\n",
        "  '                if self.y_ndim_ == 1:\\n',\n",
        "  '                    MSE = MSE.ravel()\\n',\n",
        "  '\\n',\n",
        "  '                return y, MSE\\n',\n",
        "  '\\n',\n",
        "  '            else:\\n',\n",
        "  '\\n',\n",
        "  '                return y\\n',\n",
        "  '\\n',\n",
        "  '        else:\\n',\n",
        "  '            # Memory management\\n',\n",
        "  '\\n',\n",
        "  '            if type(batch_size) is not int or batch_size <= 0:\\n',\n",
        "  '                raise Exception(\"batch_size must be a positive integer\")\\n',\n",
        "  '\\n',\n",
        "  '            if eval_MSE:\\n',\n",
        "  '\\n',\n",
        "  '                y, MSE = np.zeros(n_eval), np.zeros(n_eval)\\n',\n",
        "  '                for k in range(max(1, n_eval / batch_size)):\\n',\n",
        "  '                    batch_from = k * batch_size\\n',\n",
        "  '                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])\\n',\n",
        "  '                    y[batch_from:batch_to], MSE[batch_from:batch_to] = \\\\\\n',\n",
        "  '                        self.predict(X[batch_from:batch_to],\\n',\n",
        "  '                                     eval_MSE=eval_MSE, batch_size=None)\\n',\n",
        "  '\\n',\n",
        "  '                return y, MSE\\n',\n",
        "  '\\n',\n",
        "  '            else:\\n',\n",
        "  '\\n',\n",
        "  '                y = np.zeros(n_eval)\\n',\n",
        "  '                for k in range(max(1, n_eval / batch_size)):\\n',\n",
        "  '                    batch_from = k * batch_size\\n',\n",
        "  '                    batch_to = min([(k + 1) * batch_size + 1, n_eval + 1])\\n',\n",
        "  '                    y[batch_from:batch_to] = \\\\\\n',\n",
        "  '                        self.predict(X[batch_from:batch_to],\\n',\n",
        "  '                                     eval_MSE=eval_MSE, batch_size=None)\\n',\n",
        "  '\\n',\n",
        "  '                return y\\n',\n",
        "  '\\n',\n",
        "  '    def reduced_likelihood_function(self, theta=None):\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '        This function determines the BLUP parameters and evaluates the reduced\\n',\n",
        "  '        likelihood function for the given autocorrelation parameters theta.\\n',\n",
        "  '\\n',\n",
        "  '        Maximizing this function wrt the autocorrelation parameters theta is\\n',\n",
        "  '        equivalent to maximizing the likelihood of the assumed joint Gaussian\\n',\n",
        "  '        distribution of the observations y evaluated onto the design of\\n',\n",
        "  '        experiments X.\\n',\n",
        "  '\\n',\n",
        "  '        Parameters\\n',\n",
        "  '        ----------\\n',\n",
        "  '        theta : array_like, optional\\n',\n",
        "  '            An array containing the autocorrelation parameters at which the\\n',\n",
        "  '            Gaussian Process model parameters should be determined.\\n',\n",
        "  '            Default uses the built-in autocorrelation parameters\\n',\n",
        "  '            (ie ``theta = self.theta_``).\\n',\n",
        "  '\\n',\n",
        "  '        Returns\\n',\n",
        "  '        -------\\n',\n",
        "  '        reduced_likelihood_function_value : double\\n',\n",
        "  '            The value of the reduced likelihood function associated to the\\n',\n",
        "  '            given autocorrelation parameters theta.\\n',\n",
        "  '\\n',\n",
        "  '        par : dict\\n',\n",
        "  '            A dictionary containing the requested Gaussian Process model\\n',\n",
        "  '            parameters:\\n',\n",
        "  '\\n',\n",
        "  '                sigma2\\n',\n",
        "  '                        Gaussian Process variance.\\n',\n",
        "  '                beta\\n',\n",
        "  '                        Generalized least-squares regression weights for\\n',\n",
        "  '                        Universal Kriging or given beta0 for Ordinary\\n',\n",
        "  '                        Kriging.\\n',\n",
        "  '                gamma\\n',\n",
        "  '                        Gaussian Process weights.\\n',\n",
        "  '                C\\n',\n",
        "  '                        Cholesky decomposition of the correlation matrix [R].\\n',\n",
        "  '                Ft\\n',\n",
        "  '                        Solution of the linear equation system : [R] x Ft = F\\n',\n",
        "  '                G\\n',\n",
        "  '                        QR decomposition of the matrix Ft.\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '\\n',\n",
        "  '        if theta is None:\\n',\n",
        "  '            # Use built-in autocorrelation parameters\\n',\n",
        "  '            theta = self.theta_\\n',\n",
        "  '\\n',\n",
        "  '        # Initialize output\\n',\n",
        "  '        reduced_likelihood_function_value = - np.inf\\n',\n",
        "  '        par = {}\\n',\n",
        "  '\\n',\n",
        "  '        # Retrieve data\\n',\n",
        "  '        n_samples = self.X.shape[0]\\n',\n",
        "  '        D = self.D\\n',\n",
        "  '        ij = self.ij\\n',\n",
        "  '        F = self.F\\n',\n",
        "  '\\n',\n",
        "  '        if D is None:\\n',\n",
        "  '            # Light storage mode (need to recompute D, ij and F)\\n',\n",
        "  '            D, ij = l1_cross_distances(self.X)\\n',\n",
        "  '            if (np.min(np.sum(D, axis=1)) == 0.\\n',\n",
        "  '                    and self.corr != correlation.pure_nugget):\\n',\n",
        "  '                raise Exception(\"Multiple X are not allowed\")\\n',\n",
        "  '            F = self.regr(self.X)\\n',\n",
        "  '\\n',\n",
        "  '        # Set up R\\n',\n",
        "  '        r = self.corr(theta, D)\\n',\n",
        "  '        R = np.eye(n_samples) * (1. + self.nugget)\\n',\n",
        "  '        R[ij[:, 0], ij[:, 1]] = r\\n',\n",
        "  '        R[ij[:, 1], ij[:, 0]] = r\\n',\n",
        "  '\\n',\n",
        "  '        # Cholesky decomposition of R\\n',\n",
        "  '        try:\\n',\n",
        "  '            C = linalg.cholesky(R, lower=True)\\n',\n",
        "  '        except linalg.LinAlgError:\\n',\n",
        "  '            return reduced_likelihood_function_value, par\\n',\n",
        "  '\\n',\n",
        "  '        # Get generalized least squares solution\\n',\n",
        "  '        Ft = linalg.solve_triangular(C, F, lower=True)\\n',\n",
        "  '        try:\\n',\n",
        "  '            Q, G = linalg.qr(Ft, econ=True)\\n',\n",
        "  '        except:\\n',\n",
        "  '            #/usr/lib/python2.6/dist-packages/scipy/linalg/decomp.py:1177:\\n',\n",
        "  '            # DeprecationWarning: qr econ argument will be removed after scipy\\n',\n",
        "  '            # 0.7. The economy transform will then be available through the\\n',\n",
        "  \"            # mode='economic' argument.\\n\",\n",
        "  \"            Q, G = linalg.qr(Ft, mode='economic')\\n\",\n",
        "  '            pass\\n',\n",
        "  '\\n',\n",
        "  '        sv = linalg.svd(G, compute_uv=False)\\n',\n",
        "  '        rcondG = sv[-1] / sv[0]\\n',\n",
        "  '        if rcondG < 1e-10:\\n',\n",
        "  '            # Check F\\n',\n",
        "  '            sv = linalg.svd(F, compute_uv=False)\\n',\n",
        "  '            condF = sv[0] / sv[-1]\\n',\n",
        "  '            if condF > 1e15:\\n',\n",
        "  '                raise Exception(\"F is too ill conditioned. Poor combination \"\\n',\n",
        "  '                                \"of regression model and observations.\")\\n',\n",
        "  '            else:\\n',\n",
        "  '                # Ft is too ill conditioned, get out (try different theta)\\n',\n",
        "  '                return reduced_likelihood_function_value, par\\n',\n",
        "  '\\n',\n",
        "  '        Yt = linalg.solve_triangular(C, self.y, lower=True)\\n',\n",
        "  '        if self.beta0 is None:\\n',\n",
        "  '            # Universal Kriging\\n',\n",
        "  '            beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))\\n',\n",
        "  '        else:\\n',\n",
        "  '            # Ordinary Kriging\\n',\n",
        "  '            beta = np.array(self.beta0)\\n',\n",
        "  '\\n',\n",
        "  '        rho = Yt - np.dot(Ft, beta)\\n',\n",
        "  '        sigma2 = (rho ** 2.).sum(axis=0) / n_samples\\n',\n",
        "  '        # The determinant of R is equal to the squared product of the diagonal\\n',\n",
        "  '        # elements of its Cholesky decomposition C\\n',\n",
        "  '        detR = (np.diag(C) ** (2. / n_samples)).prod()\\n',\n",
        "  '\\n',\n",
        "  '        # Compute/Organize output\\n',\n",
        "  '        reduced_likelihood_function_value = - sigma2.sum() * detR\\n',\n",
        "  \"        par['sigma2'] = sigma2 * self.y_std ** 2.\\n\",\n",
        "  \"        par['beta'] = beta\\n\",\n",
        "  \"        par['gamma'] = linalg.solve_triangular(C.T, rho)\\n\",\n",
        "  \"        par['C'] = C\\n\",\n",
        "  \"        par['Ft'] = Ft\\n\",\n",
        "  \"        par['G'] = G\\n\",\n",
        "  '\\n',\n",
        "  '        return reduced_likelihood_function_value, par\\n',\n",
        "  '\\n',\n",
        "  '    def _arg_max_reduced_likelihood_function(self):\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '        This function estimates the autocorrelation parameters theta as the\\n',\n",
        "  '        maximizer of the reduced likelihood function.\\n',\n",
        "  '        (Minimization of the opposite reduced likelihood function is used for\\n',\n",
        "  '        convenience)\\n',\n",
        "  '\\n',\n",
        "  '        Parameters\\n',\n",
        "  '        ----------\\n',\n",
        "  '        self : All parameters are stored in the Gaussian Process model object.\\n',\n",
        "  '\\n',\n",
        "  '        Returns\\n',\n",
        "  '        -------\\n',\n",
        "  '        optimal_theta : array_like\\n',\n",
        "  '            The best set of autocorrelation parameters (the sought maximizer of\\n',\n",
        "  '            the reduced likelihood function).\\n',\n",
        "  '\\n',\n",
        "  '        optimal_reduced_likelihood_function_value : double\\n',\n",
        "  '            The optimal reduced likelihood function value.\\n',\n",
        "  '\\n',\n",
        "  '        optimal_par : dict\\n',\n",
        "  '            The BLUP parameters associated to thetaOpt.\\n',\n",
        "  '        \"\"\"\\n',\n",
        "  '\\n',\n",
        "  '        # Initialize output\\n',\n",
        "  '        best_optimal_theta = []\\n',\n",
        "  '        best_optimal_rlf_value = []\\n',\n",
        "  '        best_optimal_par = []\\n',\n",
        "  '\\n',\n",
        "  '        if self.verbose:\\n',\n",
        "  '            print(\"The chosen optimizer is: \" + str(self.optimizer))\\n',\n",
        "  '            if self.random_start > 1:\\n',\n",
        "  '                print(str(self.random_start) + \" random starts are required.\")\\n',\n",
        "  '\\n',\n",
        "  '        percent_completed = 0.\\n',\n",
        "  '\\n',\n",
        "  '        # Force optimizer to fmin_cobyla if the model is meant to be isotropic\\n',\n",
        "  \"        if self.optimizer == 'Welch' and self.theta0.size == 1:\\n\",\n",
        "  \"            self.optimizer = 'fmin_cobyla'\\n\",\n",
        "  '\\n',\n",
        "  \"        if self.optimizer == 'fmin_cobyla':\\n\",\n",
        "  '\\n',\n",
        "  '            def minus_reduced_likelihood_function(log10t):\\n',\n",
        "  '                return - self.reduced_likelihood_function(\\n',\n",
        "  '                    theta=10. ** log10t)[0]\\n',\n",
        "  '\\n',\n",
        "  '            constraints = []\\n',\n",
        "  '            for i in range(self.theta0.size):\\n',\n",
        "  '                constraints.append(lambda log10t, i=i:\\n',\n",
        "  '                                   log10t[i] - np.log10(self.thetaL[0, i]))\\n',\n",
        "  '                constraints.append(lambda log10t, i=i:\\n',\n",
        "  '                                   np.log10(self.thetaU[0, i]) - log10t[i])\\n',\n",
        "  '\\n',\n",
        "  '            for k in range(self.random_start):\\n',\n",
        "  '\\n',\n",
        "  '                if k == 0:\\n',\n",
        "  '                    # Use specified starting point as first guess\\n',\n",
        "  '                    theta0 = self.theta0\\n',\n",
        "  '                else:\\n',\n",
        "  '                    # Generate a random starting point log10-uniformly\\n',\n",
        "  '                    # distributed between bounds\\n',\n",
        "  '                    log10theta0 = np.log10(self.thetaL) \\\\\\n',\n",
        "  '                        + self.random_state.rand(self.theta0.size).reshape(\\n',\n",
        "  '                            self.theta0.shape) * np.log10(self.thetaU\\n',\n",
        "  '                                                          / self.thetaL)\\n',\n",
        "  '                    theta0 = 10. ** log10theta0\\n',\n",
        "  '\\n',\n",
        "  '                # Run Cobyla\\n',\n",
        "  '                try:\\n',\n",
        "  '                    log10_optimal_theta = \\\\\\n',\n",
        "  '                        optimize.fmin_cobyla(minus_reduced_likelihood_function,\\n',\n",
        "  '                                             np.log10(theta0), constraints,\\n',\n",
        "  '                                             iprint=0)\\n',\n",
        "  '                except ValueError as ve:\\n',\n",
        "  '                    print(\"Optimization failed. Try increasing the ``nugget``\")\\n',\n",
        "  '                    raise ve\\n',\n",
        "  '\\n',\n",
        "  '                optimal_theta = 10. ** log10_optimal_theta\\n',\n",
        "  '                optimal_rlf_value, optimal_par = \\\\\\n',\n",
        "  '                    self.reduced_likelihood_function(theta=optimal_theta)\\n',\n",
        "  '\\n',\n",
        "  '                # Compare the new optimizer to the best previous one\\n',\n",
        "  '                if k > 0:\\n',\n",
        "  '                    if optimal_rlf_value > best_optimal_rlf_value:\\n',\n",
        "  '                        best_optimal_rlf_value = optimal_rlf_value\\n',\n",
        "  '                        best_optimal_par = optimal_par\\n',\n",
        "  '                        best_optimal_theta = optimal_theta\\n',\n",
        "  '                else:\\n',\n",
        "  '                    best_optimal_rlf_value = optimal_rlf_value\\n',\n",
        "  '                    best_optimal_par = optimal_par\\n',\n",
        "  '                    best_optimal_theta = optimal_theta\\n',\n",
        "  '                if self.verbose and self.random_start > 1:\\n',\n",
        "  '                    if (20 * k) / self.random_start > percent_completed:\\n',\n",
        "  '                        percent_completed = (20 * k) / self.random_start\\n',\n",
        "  '                        print(\"%s completed\" % (5 * percent_completed))\\n',\n",
        "  '\\n',\n",
        "  '            optimal_rlf_value = best_optimal_rlf_value\\n',\n",
        "  '            optimal_par = best_optimal_par\\n',\n",
        "  '            optimal_theta = best_optimal_theta\\n',\n",
        "  '\\n',\n",
        "  \"        elif self.optimizer == 'Welch':\\n\",\n",
        "  '\\n',\n",
        "  '            # Backup of the given atrributes\\n',\n",
        "  '            theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU\\n',\n",
        "  '            corr = self.corr\\n',\n",
        "  '            verbose = self.verbose\\n',\n",
        "  '\\n',\n",
        "  '            # This will iterate over fmin_cobyla optimizer\\n',\n",
        "  \"            self.optimizer = 'fmin_cobyla'\\n\",\n",
        "  '            self.verbose = False\\n',\n",
        "  '\\n',\n",
        "  '            # Initialize under isotropy assumption\\n',\n",
        "  '            if verbose:\\n',\n",
        "  '                print(\"Initialize under isotropy assumption...\")\\n',\n",
        "  '            self.theta0 = array2d(self.theta0.min())\\n',\n",
        "  '            self.thetaL = array2d(self.thetaL.min())\\n',\n",
        "  '            self.thetaU = array2d(self.thetaU.max())\\n',\n",
        "  '            theta_iso, optimal_rlf_value_iso, par_iso = \\\\\\n',\n",
        "  '                self._arg_max_reduced_likelihood_function()\\n',\n",
        "  '            optimal_theta = theta_iso + np.zeros(theta0.shape)\\n',\n",
        "  '\\n',\n",
        "  '            # Iterate over all dimensions of theta allowing for anisotropy\\n',\n",
        "  '            if verbose:\\n',\n",
        "  '                print(\"Now improving allowing for anisotropy...\")\\n',\n",
        "  '            for i in self.random_state.permutation(theta0.size):\\n',\n",
        "  '                if verbose:\\n',\n",
        "  '                    print(\"Proceeding along dimension %d...\" % (i + 1))\\n',\n",
        "  '                self.theta0 = array2d(theta_iso)\\n',\n",
        "  '                self.thetaL = array2d(thetaL[0, i])\\n',\n",
        "  '                self.thetaU = array2d(thetaU[0, i])\\n',\n",
        "  '\\n',\n",
        "  '                def corr_cut(t, d):\\n',\n",
        "  '                    return corr(array2d(np.hstack([optimal_theta[0][0:i],\\n',\n",
        "  '                                                   t[0],\\n',\n",
        "  '                                                   optimal_theta[0][(i + 1)::]]\\n',\n",
        "  '                                                  )), d)\\n',\n",
        "  '\\n',\n",
        "  '                self.corr = corr_cut\\n',\n",
        "  '                optimal_theta[0, i], optimal_rlf_value, optimal_par = \\\\\\n',\n",
        "  '                    self._arg_max_reduced_likelihood_function()\\n',\n",
        "  '\\n',\n",
        "  '            # Restore the given atrributes\\n',\n",
        "  '            self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU\\n',\n",
        "  '            self.corr = corr\\n',\n",
        "  \"            self.optimizer = 'Welch'\\n\",\n",
        "  '            self.verbose = verbose\\n',\n",
        "  '\\n',\n",
        "  '        else:\\n',\n",
        "  '\\n',\n",
        "  '            raise NotImplementedError(\"This optimizer (\\'%s\\') is not \"\\n',\n",
        "  '                                      \"implemented yet. Please contribute!\"\\n',\n",
        "  '                                      % self.optimizer)\\n',\n",
        "  '\\n',\n",
        "  '        return optimal_theta, optimal_rlf_value, optimal_par\\n',\n",
        "  '\\n',\n",
        "  '    def _check_params(self, n_samples=None):\\n',\n",
        "  '\\n',\n",
        "  '        # Check regression model\\n',\n",
        "  '        if not callable(self.regr):\\n',\n",
        "  '            if self.regr in self._regression_types:\\n',\n",
        "  '                self.regr = self._regression_types[self.regr]\\n',\n",
        "  '            else:\\n',\n",
        "  '                raise ValueError(\"regr should be one of %s or callable, \"\\n',\n",
        "  '                                 \"%s was given.\"\\n',\n",
        "  '                                 % (self._regression_types.keys(), self.regr))\\n',\n",
        "  '\\n',\n",
        "  '        # Check regression weights if given (Ordinary Kriging)\\n',\n",
        "  '        if self.beta0 is not None:\\n',\n",
        "  '            self.beta0 = array2d(self.beta0)\\n',\n",
        "  '            if self.beta0.shape[1] != 1:\\n',\n",
        "  '                # Force to column vector\\n',\n",
        "  '                self.beta0 = self.beta0.T\\n',\n",
        "  '\\n',\n",
        "  '        # Check correlation model\\n',\n",
        "  '        if not callable(self.corr):\\n',\n",
        "  '            if self.corr in self._correlation_types:\\n',\n",
        "  '                self.corr = self._correlation_types[self.corr]\\n',\n",
        "  '            else:\\n',\n",
        "  '                raise ValueError(\"corr should be one of %s or callable, \"\\n',\n",
        "  '                                 \"%s was given.\"\\n',\n",
        "  '                                 % (self._correlation_types.keys(), self.corr))\\n',\n",
        "  '\\n',\n",
        "  '        # Check storage mode\\n',\n",
        "  \"        if self.storage_mode != 'full' and self.storage_mode != 'light':\\n\",\n",
        "  '            raise ValueError(\"Storage mode should either be \\'full\\' or \"\\n',\n",
        "  '                             \"\\'light\\', %s was given.\" % self.storage_mode)\\n',\n",
        "  '\\n',\n",
        "  '        # Check correlation parameters\\n',\n",
        "  '        self.theta0 = array2d(self.theta0)\\n',\n",
        "  '        lth = self.theta0.size\\n',\n",
        "  '\\n',\n",
        "  '        if self.thetaL is not None and self.thetaU is not None:\\n',\n",
        "  '            self.thetaL = array2d(self.thetaL)\\n',\n",
        "  '            self.thetaU = array2d(self.thetaU)\\n',\n",
        "  '            if self.thetaL.size != lth or self.thetaU.size != lth:\\n',\n",
        "  '                raise ValueError(\"theta0, thetaL and thetaU must have the \"\\n',\n",
        "  '                                 \"same length.\")\\n',\n",
        "  '            if np.any(self.thetaL <= 0) or np.any(self.thetaU < self.thetaL):\\n',\n",
        "  '                raise ValueError(\"The bounds must satisfy O < thetaL <= \"\\n',\n",
        "  '                                 \"thetaU.\")\\n',\n",
        "  '\\n',\n",
        "  '        elif self.thetaL is None and self.thetaU is None:\\n',\n",
        "  '            if np.any(self.theta0 <= 0):\\n',\n",
        "  '                raise ValueError(\"theta0 must be strictly positive.\")\\n',\n",
        "  '\\n',\n",
        "  '        elif self.thetaL is None or self.thetaU is None:\\n',\n",
        "  '            raise ValueError(\"thetaL and thetaU should either be both or \"\\n',\n",
        "  '                             \"neither specified.\")\\n',\n",
        "  '\\n',\n",
        "  '        # Force verbose type to bool\\n',\n",
        "  '        self.verbose = bool(self.verbose)\\n',\n",
        "  '\\n',\n",
        "  '        # Force normalize type to bool\\n',\n",
        "  '        self.normalize = bool(self.normalize)\\n',\n",
        "  '\\n',\n",
        "  '        # Check nugget value\\n',\n",
        "  '        self.nugget = np.asarray(self.nugget)\\n',\n",
        "  '        if np.any(self.nugget) < 0.:\\n',\n",
        "  '            raise ValueError(\"nugget must be positive or zero.\")\\n',\n",
        "  '        if (n_samples is not None\\n',\n",
        "  '                and self.nugget.shape not in [(), (n_samples,)]):\\n',\n",
        "  '            raise ValueError(\"nugget must be either a scalar \"\\n',\n",
        "  '                             \"or array of length n_samples.\")\\n',\n",
        "  '\\n',\n",
        "  '        # Check optimizer\\n',\n",
        "  '        if not self.optimizer in self._optimizer_types:\\n',\n",
        "  '            raise ValueError(\"optimizer should be one of %s\"\\n',\n",
        "  '                             % self._optimizer_types)\\n',\n",
        "  '\\n',\n",
        "  '        # Force random_start type to int\\n',\n",
        "  '        self.random_start = int(self.random_start)\\n'],\n",
        " 58)"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_process.__file__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "'/usr/local/lib/python2.7/dist-packages/sklearn/gaussian_process/__init__.pyc'"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_process.GaussianProcess."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "type object 'GaussianProcess' has no attribute '__file__'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-48-a2f4d676614e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgaussian_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: type object 'GaussianProcess' has no attribute '__file__'"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "y.ravel() \u8868\u793a\u4ec0\u4e48\u610f\u601d\uff1f"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred, sigma2_pred = gp.predict(x,eval_MSE= True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u663e\u793a\u7ed3\u679c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(X,y,'r--',label='test data')\n",
      "plt.plot(x,y_pred,'g*--',label='predicted result')\n",
      "plt.plot(x,f(x),'y^--',label='real function')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "<matplotlib.legend.Legend at 0x7fb0985bfd90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fb098ba8c90>"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u8bf4\u660e\u8fd9\u4e2a\u51fd\u6570\u662f\u6709\u6548\u7684"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    }
   ],
   "metadata": {}
  }
 ]
}